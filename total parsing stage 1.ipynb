{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef5be4-10cd-4b76-b199-c22bbd8f2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "the developing code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c31ba67-85e9-412f-9b4f-dfdbdcd51bd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed Output from: 6 word resume.docx\n",
      "\n",
      "================ RAW PARSED TEXT BLOCKS ================\n",
      "\n",
      "Sample Resume\n",
      "----------------------------------------\n",
      "SANJAY GOPAL\n",
      "----------------------------------------\n",
      "54 Dunster Street    Cambridge, MA 02138   555-555-5555    you@gmail.com\n",
      "----------------------------------------\n",
      "Project Director\n",
      "----------------------------------------\n",
      "Project Director with extensive leadership experience in highly competitive IT and Telecom industry. Proven track record of leading and managing multi-million-dollar international programs across northern Europe, Middle East, North America and South America.\n",
      "----------------------------------------\n",
      "Specialize in launching new services and products from concept to roll-out and building organizations from ground up. Expertise in improving team performance while securing customer loyalty and forging valuable relationships with internal and external partners.\n",
      "----------------------------------------\n",
      "Core Competencies\n",
      "----------------------------------------\n",
      "• \tProject/Operations Management \t• \tLeadership \t•\n",
      "----------------------------------------\n",
      "P&L Management\n",
      "----------------------------------------\n",
      "• \tStrategic Planning \t• \tBuilding organizations \t•\n",
      "----------------------------------------\n",
      "Risk Management\n",
      "----------------------------------------\n",
      "• \tClient Management & Retention \t• \tNegotiations \t•\n",
      "----------------------------------------\n",
      "Business development\n",
      "----------------------------------------\n",
      "Professional Experience\n",
      "----------------------------------------\n",
      "Comyerse Inc., USA \t \tOct 20XX - Present\n",
      "----------------------------------------\n",
      "The world's leading provider of Telecom software and systems\n",
      "----------------------------------------\n",
      "Project Director / Consulting Program Manager, Boston / London / Dubai\n",
      "----------------------------------------\n",
      "Delivered 30+ projects and programs within agreed budget, time, and quality for telecom operators in North America, South America, northern Europe and Middle East region. Planned & supervised “concept to launch” for enterprise software systems, system integration projects for telecom operators in voice, data & billing domains. Prepared service proposals, RFP responses & worked closely with sales groups to secure new business.\n",
      "----------------------------------------\n",
      "Accomplishments:\n",
      "----------------------------------------\n",
      "--\tDelivered 30+ projects valued at 80+ Million USD, on-time, within budget with team of up to 100 people for Verizon Wireless, Sprint, Bell Canada (North American clients), America-Movil, Millicom (South American clients), Vodafone, Orange, (European Clients), Q-Tel and Etisalat (Middle Eastern clients).\n",
      "----------------------------------------\n",
      "--\tIntroduced Visual Voice Mail services for Verizon wireless nationwide in aggressive schedule with team of 100+ professionals.\n",
      "----------------------------------------\n",
      "--\tManaged launch of first Ring Back tone project for Sprint within very demanding timeframe. Comverse was awarded multiple expansions based on success of project.\n",
      "----------------------------------------\n",
      "--\tCoordinated very competitive trials for multiple services for Bell Canada and won the contract.\n",
      "----------------------------------------\n",
      "--\tLaunched a globally distributed ring back tone service for Orange Global in UK, France and Belgium.\n",
      "----------------------------------------\n",
      "Team consisted of 100+ team members including Sub-contractor (Cap-Gemini).\n",
      "----------------------------------------\n",
      "--\tIntroduced new product lines across North America, Europe and Latin America.\n",
      "----------------------------------------\n",
      "--\tBuilt and managed Comyerse (Middle East) organization from scratch to team of 4 Project Managers and 13 Engineers.\n",
      "----------------------------------------\n",
      "Sample Resume (page 2)\n",
      "----------------------------------------\n",
      "Atlas Telecommunications, UAE \tApril 20XX - Sept 20XX\n",
      "----------------------------------------\n",
      "Leading telecomm solution & system supplier, based in UAE\n",
      "----------------------------------------\n",
      "Business Development Manager, Abu-Dhabi\n",
      "----------------------------------------\n",
      "Marketed and sold telecommunication systems for Telecom, Defense, Oil and gas companies. Managed contract negotiations, RFI /RFP responses and project agreements.\n",
      "----------------------------------------\n",
      "Accomplishments:\n",
      "----------------------------------------\n",
      "--\tExceeded the sales target for 20XX and 20XX by 25% (3.6 Million USD).\n",
      "----------------------------------------\n",
      "--\tSuccessfully introduced and won projects for Mera systems, Scientific South and Comyerse Inc.\n",
      "----------------------------------------\n",
      "Facile Call Paging, India \tJune 20XX - March 20XX\n",
      "----------------------------------------\n",
      "Largest & most innovative paging service provider in India\n",
      "----------------------------------------\n",
      "Sr. Manager (Projects & Operations), New Delhi\n",
      "----------------------------------------\n",
      "Launched first green field paging network across north India. Managed operations and customer support with team of 9 engineers and 70 customer care agents.\n",
      "----------------------------------------\n",
      "Accomplishments:\n",
      "----------------------------------------\n",
      "--\tBuilt Facile Call technical organization from ground up across 7 locations in India.\n",
      "----------------------------------------\n",
      "--\tMember of core team to bid nationwide spectrum auctions and vendor selection.\n",
      "----------------------------------------\n",
      "--\tLaunched and managed green field paging services across major cities in demanding time scales.\n",
      "----------------------------------------\n",
      "Education\n",
      "----------------------------------------\n",
      "Harvard University Extension School, Master of Liberal Arts, Management, Expected May 20XX\n",
      "----------------------------------------\n",
      "Regional Engineering College, Surat, India, Bachelor of Engineering, May 20XX\n",
      "----------------------------------------\n",
      "Project Management Institute (PMI), Professional Certification: PMP\n",
      "----------------------------------------\n",
      "Sanjay Gopal \t[2]\n",
      "----------------------------------------\n",
      "\n",
      "================ SEMANTIC SECTIONING OUTPUT ================\n",
      "\n",
      "\n",
      "### PROJECT DIRECTOR ###\n",
      "Project Director with extensive leadership experience in highly competitive IT and Telecom industry. Proven track record of leading and managing multi-million-dollar international programs across northern Europe, Middle East, North America and South America.\n",
      "Specialize in launching new services and products from concept to roll-out and building organizations from ground up. Expertise in improving team performance while securing customer loyalty and forging valuable relationships with internal and external partners.\n",
      "\n",
      "============================================================\n",
      "\n",
      "### CORE COMPETENCIES ###\n",
      "• \tProject/Operations Management \t• \tLeadership \t•\n",
      "P&L Management\n",
      "• \tStrategic Planning \t• \tBuilding organizations \t•\n",
      "Risk Management\n",
      "• \tClient Management & Retention \t• \tNegotiations \t•\n",
      "Business development\n",
      "\n",
      "============================================================\n",
      "\n",
      "### PROFESSIONAL EXPERIENCE ###\n",
      "Comyerse Inc., USA \t \tOct 20XX - Present\n",
      "The world's leading provider of Telecom software and systems\n",
      "Project Director / Consulting Program Manager, Boston / London / Dubai\n",
      "Delivered 30+ projects and programs within agreed budget, time, and quality for telecom operators in North America, South America, northern Europe and Middle East region. Planned & supervised “concept to launch” for enterprise software systems, system integration projects for telecom operators in voice, data & billing domains. Prepared service proposals, RFP responses & worked closely with sales groups to secure new business.\n",
      "\n",
      "============================================================\n",
      "\n",
      "### ACCOMPLISHMENTS: ###\n",
      "--\tBuilt Facile Call technical organization from ground up across 7 locations in India.\n",
      "--\tMember of core team to bid nationwide spectrum auctions and vendor selection.\n",
      "--\tLaunched and managed green field paging services across major cities in demanding time scales.\n",
      "\n",
      "============================================================\n",
      "\n",
      "### EDUCATION ###\n",
      "Harvard University Extension School, Master of Liberal Arts, Management, Expected May 20XX\n",
      "Regional Engineering College, Surat, India, Bachelor of Engineering, May 20XX\n",
      "Project Management Institute (PMI), Professional Certification: PMP\n",
      "Sanjay Gopal \t[2]\n",
      "\n",
      "============================================================\n",
      "\n",
      "================ NOUN CHUNKS PER SECTION ================\n",
      "\n",
      "\n",
      "### PROJECT DIRECTOR ###\n",
      "project director, extensive leadership experience, highly competitive it and telecom industry, proven track record, multi-million-dollar international programs, northern europe, middle east, north america, south america, new services, products, concept, organizations, ground, expertise, team performance, customer loyalty, valuable relationships, internal and external partners\n",
      "============================================================\n",
      "\n",
      "### CORE COMPETENCIES ###\n",
      "• \tproject/operations management \t• \tleadership, p&l management, strategic planning\n",
      "============================================================\n",
      "\n",
      "### PROFESSIONAL EXPERIENCE ###\n",
      "comyerse inc., the world's leading provider, telecom software and systems project director / consulting program manager, boston / london / dubai, + projects, programs, agreed budget, time, quality, telecom operators, north america, south america, northern europe, middle east region, concept, enterprise software systems, system integration projects, voice, data, billing domains, prepared service proposals, rfp responses, sales groups, new business\n",
      "============================================================\n",
      "\n",
      "### ACCOMPLISHMENTS: ###\n",
      "--\tbuilt facile call technical organization, ground, 7 locations, india, member, core team, nationwide spectrum auctions, vendor selection, services, major cities, time scales\n",
      "============================================================\n",
      "\n",
      "### EDUCATION ###\n",
      "harvard university extension school, master, liberal arts, management, may 20xx, regional engineering college, engineering, project management institute, pmi, professional certification, pmp sanjay gopal\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pymupdf4llm\n",
    "from docx2python import docx2python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "\n",
    "# ================================\n",
    "# UNIVERSAL TEXT EXTRACTION\n",
    "# ================================\n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_pdf_blocks(file_path)\n",
    "    elif ext in [\".docx\", \".doc\"]:\n",
    "        return extract_docx_exact_layout(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Use PDF or DOCX.\")\n",
    "\n",
    "def extract_docx_exact_layout(filepath):\n",
    "    doc_result = docx2python(filepath)\n",
    "    main_content = doc_result.body\n",
    "    all_text = []\n",
    "    for section in main_content:\n",
    "        for row in section:\n",
    "            for cell in row:\n",
    "                for para in cell:\n",
    "                    para_str = para.strip()\n",
    "                    if para_str:\n",
    "                        all_text.append(para_str)\n",
    "    return all_text\n",
    "\n",
    "def extract_column_aware_blocks(pdf_path, column_gap=50):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_blocks = []\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"blocks\", sort=True)\n",
    "        blocks = sorted(blocks, key=lambda b: (b[0], b[1]))\n",
    "        left_col, right_col = [], []\n",
    "        if blocks:\n",
    "            page_width = page.rect.width\n",
    "            center_line = page_width / 2\n",
    "            for b in blocks:\n",
    "                x0, y0, x1, y1, text, *_ = b\n",
    "                if x1 < center_line - column_gap:\n",
    "                    left_col.append((y0, text.strip()))\n",
    "                else:\n",
    "                    right_col.append((y0, text.strip()))\n",
    "            left_col_sorted = [t for _, t in sorted(left_col)]\n",
    "            right_col_sorted = [t for _, t in sorted(right_col)]\n",
    "            combined = right_col_sorted + left_col_sorted\n",
    "            all_blocks.extend([t for t in combined if t])\n",
    "    doc.close()\n",
    "    return all_blocks\n",
    "\n",
    "def extract_pdf_blocks(pdf_path):\n",
    "    blocks_code1 = extract_column_aware_blocks(pdf_path)\n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "    blocks_code2 = [\n",
    "        block.strip()\n",
    "        for block in md_text.split(\"\\n\\n\")\n",
    "        if block.strip() and len(block.strip()) > 5\n",
    "    ]\n",
    "    md_chunks = pymupdf4llm.to_markdown(pdf_path, page_chunks=True)\n",
    "    page_chunk_blocks = [\n",
    "        chunk[\"text\"].strip()\n",
    "        for chunk in md_chunks\n",
    "        if isinstance(chunk, dict) and \"text\" in chunk and chunk[\"text\"].strip()\n",
    "    ]\n",
    "\n",
    "    def jaccard_similarity(s1, s2, threshold=0.7):\n",
    "        set1, set2 = set(s1.lower().strip()), set(s2.lower().strip())\n",
    "        intersection = set1 & set2\n",
    "        union = set1 | set2\n",
    "        if not union:\n",
    "            return False\n",
    "        return len(intersection) / len(union) > threshold\n",
    "\n",
    "    missing_from_code2 = []\n",
    "    for block1 in blocks_code1:\n",
    "        if not any(jaccard_similarity(block1, block2) for block2 in blocks_code2):\n",
    "            missing_from_code2.append(block1)\n",
    "    final_blocks = blocks_code2 + missing_from_code2\n",
    "    return final_blocks, page_chunk_blocks\n",
    "\n",
    "# ================================\n",
    "# SEMANTIC HEADING DETECTION + SECTION GROUPING\n",
    "# ================================\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "canonical_headings = [\n",
    "    \"experience\", \"work experience\", \"education\", \"academic history\",\n",
    "    \"projects\", \"technical projects\", \"skills\", \"key skills\",\n",
    "    \"certifications\", \"achievements\", \"publications\", \"personal details\", \"contact\",\n",
    "    \"technical skills\", \"professional experience\",\"Core competencies\"\n",
    "]\n",
    "canonical_embeddings = model.encode(canonical_headings, convert_to_tensor=True)\n",
    "\n",
    "def semantic_heading_detection(lines, threshold=0.6):\n",
    "    headings = []\n",
    "    for line in lines:\n",
    "        line_clean = line.strip().lower()\n",
    "        if not line_clean or len(line_clean) < 3:\n",
    "            continue\n",
    "        emb = model.encode(line_clean, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(emb, canonical_embeddings)[0]\n",
    "        max_score = float(cosine_scores.max())\n",
    "        if max_score >= threshold:\n",
    "            headings.append(line.strip())\n",
    "    return headings\n",
    "\n",
    "def semantic_sectioning(lines, threshold=0.55):\n",
    "    detected_headings = semantic_heading_detection(lines, threshold)\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    buffer = []\n",
    "    for line in lines:\n",
    "        if line.strip() in detected_headings:\n",
    "            if current_section and buffer:\n",
    "                sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "            current_section = line.strip()\n",
    "            buffer = []\n",
    "        else:\n",
    "            buffer.append(line.strip())\n",
    "    if current_section and buffer:\n",
    "        sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "    return sections\n",
    "\n",
    "# ================================\n",
    "# SPA-CY FEATURE EXTRACTION\n",
    "# ================================\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def filter_and_clean_noun_chunks(doc):\n",
    "    seen = set()\n",
    "    clean_chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        text = chunk.text.strip().lower()\n",
    "        if not text or len(text) < 2:\n",
    "            continue\n",
    "        if all(token.is_stop for token in chunk):\n",
    "            continue\n",
    "        if text in seen:\n",
    "            continue\n",
    "        seen.add(text)\n",
    "        clean_chunks.append(text)\n",
    "    return clean_chunks\n",
    "\n",
    "def extract_section_spacy_features(sections):\n",
    "    section_features = {}\n",
    "    for heading, text in sections.items():\n",
    "        flat = \" \".join(line.strip() for line in text.splitlines() if line.strip())\n",
    "        doc = nlp(flat)\n",
    "        noun_chunks = filter_and_clean_noun_chunks(doc)\n",
    "        verbs = sorted(set([t.lemma_ for t in doc if t.pos_ == \"VERB\" and not t.is_stop and len(t.lemma_) > 1]))\n",
    "        compounds = []\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if any(t.dep_ == \"compound\" for t in chunk):\n",
    "                compound_text = chunk.text.strip().lower()\n",
    "                if compound_text not in compounds:\n",
    "                    compounds.append(compound_text)\n",
    "        verbal_nouns = sorted(set([t.text for t in doc if t.tag_ == \"VBG\" and t.pos_ == \"NOUN\"]))\n",
    "        dates = sorted(set([ent.text for ent in doc.ents if ent.label_ == \"DATE\"]))\n",
    "        section_features[heading] = {\n",
    "            \"noun_chunks\": noun_chunks,\n",
    "            \"compounds\": compounds,\n",
    "            \"verbal_nouns\": verbal_nouns,\n",
    "            \"verbs\": verbs,\n",
    "            \"dates\": dates\n",
    "        }\n",
    "    return section_features\n",
    "\n",
    "# ================================\n",
    "# MAIN - ORDERED TAGGED EXTRACT\n",
    "# ================================\n",
    "def tag_section_features_in_order(sections, section_features):\n",
    "    for heading, text in sections.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\\n\")\n",
    "        # For each sentence/line, scan for presence of feature and tag for each word/phrase\n",
    "        lines = text.splitlines()\n",
    "        features = section_features[heading]\n",
    "        \n",
    "        # Get all unique possible feature strings (lowercase for matching)\n",
    "        feat_types = ['noun_chunks', 'compounds', 'verbal_nouns', 'verbs', 'dates']\n",
    "        feat_map = {}\n",
    "        for ft in feat_types:\n",
    "            for val in features[ft]:\n",
    "                val_low = val.lower().strip()\n",
    "                if val_low not in feat_map:\n",
    "                    feat_map[val_low] = []\n",
    "                feat_map[val_low].append(ft)\n",
    "        \n",
    "        # For each line, scan word-by-word, tag feature matches (longest first to avoid substrings issue)\n",
    "        import re\n",
    "        # Collect all feature phrases, sorted by length descending\n",
    "        phrases_sorted = sorted(feat_map.keys(), key=lambda x: -len(x))\n",
    "        for line in lines:\n",
    "            line_str = line.strip()\n",
    "            output = line_str\n",
    "            for phrase in phrases_sorted:\n",
    "                if phrase and phrase in line_str.lower():\n",
    "                    tag = ','.join(feat_map[phrase])\n",
    "                    # Use word boundary for more precise matching\n",
    "                    pat = r'(?i)\\b({})\\b'.format(re.escape(phrase))\n",
    "                    output = re.sub(pat, r'[\\1|{}]'.format(tag), output)\n",
    "            print(output)\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"3372246-student-nurse-resume-with-clinical-experience.pdf\"\n",
    "    extracted_output = extract_text(file_path)\n",
    "\n",
    "    if isinstance(extracted_output, tuple):\n",
    "        final_blocks, page_chunks = extracted_output\n",
    "    else:\n",
    "        final_blocks = extracted_output\n",
    "        page_chunks = []\n",
    "\n",
    "    print(f\"\\nParsed Output from: {os.path.basename(file_path)}\")\n",
    "    print(\"\\n================ RAW PARSED TEXT BLOCKS ================\\n\")\n",
    "    for block in final_blocks:\n",
    "        print(block)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n================ SEMANTIC SECTIONING OUTPUT ================\\n\")\n",
    "    sections = semantic_sectioning(final_blocks)\n",
    "    for heading, content in sections.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\\n{content}\\n\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n================ SPA-CY FEATURES PER SECTION ================\\n\")\n",
    "    section_features = extract_section_spacy_features(sections)\n",
    "    for heading, feats in section_features.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\")\n",
    "        print(\"Noun Chunks: \", \", \".join(feats[\"noun_chunks\"]))\n",
    "        print(\"Compound Nouns: \", \", \".join(feats[\"compounds\"]))\n",
    "        print(\"Verbal Nouns: \", \", \".join(feats[\"verbal_nouns\"]))\n",
    "        print(\"Verbs: \", \", \".join(feats[\"verbs\"]))\n",
    "        print(\"Dates: \", \", \".join(feats[\"dates\"]))\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n================ ORDERED TAGGED SEMANTIC EXTRACT ================\\n\")\n",
    "    tag_section_features_in_order(sections, section_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86e65ade-d81c-44d4-a3a9-683fc90fb96f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed Output from: 1 word resume.docx\n",
      "\n",
      "================ RAW PARSED TEXT BLOCKS ================\n",
      "\n",
      "Sample Resume\n",
      "----------------------------------------\n",
      "Jin Wang\n",
      "----------------------------------------\n",
      "email@gmail.com • (555) 555-5555\n",
      "----------------------------------------\n",
      "Education\n",
      "----------------------------------------\n",
      "Harvard University, Extension School\n",
      "----------------------------------------\n",
      "Master of Liberal Arts, Information Management Systems GPA 4.0\n",
      "----------------------------------------\n",
      "--\tClass Marshall Award\n",
      "----------------------------------------\n",
      "--\tDean’s List Academic Achievement Award\n",
      "----------------------------------------\n",
      "--\tData Science Project: Financial Market Analysis Using Machine Learning\n",
      "----------------------------------------\n",
      "--\tCapstone Project: Enterprise Data Lake\n",
      "----------------------------------------\n",
      "University of Malaya\n",
      "----------------------------------------\n",
      "Bachelor of Computer Science\n",
      "----------------------------------------\n",
      "Technical Skills\n",
      "----------------------------------------\n",
      "Harvard University, Extension School\n",
      "----------------------------------------\n",
      "Master of Liberal Arts, Information Management Systems GPA 4.0\n",
      "----------------------------------------\n",
      "--\tClass Marshall Award\n",
      "----------------------------------------\n",
      "--\tDean’s List Academic Achievement Award\n",
      "----------------------------------------\n",
      "--\tData Science Project: Financial Market Analysis Using Machine Learning\n",
      "----------------------------------------\n",
      "--\tCapstone Project: Enterprise Data Lake\n",
      "----------------------------------------\n",
      "University of Malaya\n",
      "----------------------------------------\n",
      "Bachelor of Computer Science\n",
      "----------------------------------------\n",
      "Technical Skills\n",
      "----------------------------------------\n",
      "May 20XX\n",
      "----------------------------------------\n",
      "June 20XX\n",
      "----------------------------------------\n",
      "• Machine Learning \t• Python/Scikit-learn\n",
      "----------------------------------------\n",
      "• Spark\n",
      "----------------------------------------\n",
      "• Data Visualization\n",
      "----------------------------------------\n",
      "• Quantitative Analysis • Cloud Computing\n",
      "----------------------------------------\n",
      "• Hadoop\n",
      "----------------------------------------\n",
      "• Java/C#\n",
      "----------------------------------------\n",
      "• Unix Scripting \t• Oracle/SQL Server\n",
      "----------------------------------------\n",
      "• PLSQL/T-SQL\n",
      "----------------------------------------\n",
      "• Data Warehouse/ETL\n",
      "----------------------------------------\n",
      "• RDBMS Tuning \t• Network Protocols\n",
      "----------------------------------------\n",
      "Profession\n",
      "----------------------------------------\n",
      "• Agile & DevOps\n",
      "----------------------------------------\n",
      "al Experience\n",
      "----------------------------------------\n",
      "• Web Development\n",
      "----------------------------------------\n",
      "Rande Corporate & Investment Banking \tDetroit, MI\n",
      "----------------------------------------\n",
      "Associate – Information Technology \tSeptember 20XX – Present\n",
      "----------------------------------------\n",
      "--\tLead a team of six people to manage, operate, and support low latency posttrade brokerage platform\n",
      "----------------------------------------\n",
      "--\tImproved the performance of straight-through processing by tuning database applications\n",
      "----------------------------------------\n",
      "--\tReduced number of major incidents by 23% through problem management\n",
      "----------------------------------------\n",
      "--\tAutomate manual back-office processing through scripting and automation engine\n",
      "----------------------------------------\n",
      "--\tActively participate in and contribute to internal data science project initiatives\n",
      "----------------------------------------\n",
      "Olson Financial \tSingapore\n",
      "----------------------------------------\n",
      "Associate – Information Technology                                           February 20XX – September 20XX\n",
      "----------------------------------------\n",
      "--\tBuilt a new application support team of five people focusing on post-trading straight- through processing and data warehouse extract-transform-load processing\n",
      "----------------------------------------\n",
      "--\tDesigned and implemented global application monitoring platform.\n",
      "----------------------------------------\n",
      "--\tEliminated 80% of manual checks for trading support, and decreased SLA breaches for client reporting by 15%\n",
      "----------------------------------------\n",
      "Sample Resume (page 2)\n",
      "----------------------------------------\n",
      "PS Engineering Information Ltd. \tSingapore\n",
      "----------------------------------------\n",
      "Software Developer – Technology Office \tJuly 20XX – January 20XX\n",
      "----------------------------------------\n",
      "--\tBuilt Command & Control System for Singapore Civil Defence Force using C# .NET WCF Services\n",
      "----------------------------------------\n",
      "--\tIntegrated proprietary software components with commercial off-the-shell software product\n",
      "----------------------------------------\n",
      "Well \tBeijing, China\n",
      "----------------------------------------\n",
      "Software Developer \tJune 20XX – June 20XX\n",
      "----------------------------------------\n",
      "--\tBuilt supply chain management system using Java Spring/Hibernate Framework and Service Oriented Architecture\n",
      "----------------------------------------\n",
      "--\tImproved the performance of real-time business activity monitoring report and reduce the report response time by more than 50%\n",
      "----------------------------------------\n",
      "Silver Technologies Ltd. \tSingapore\n",
      "----------------------------------------\n",
      "Software Developer \tMay 20XX – May 20XX\n",
      "----------------------------------------\n",
      "--\tDeveloped web-based Point of Sale (POS) application using C# .NET for a multinational fashion retailor\n",
      "----------------------------------------\n",
      "--\tResearched and implemented RFID authentication software module\n",
      "----------------------------------------\n",
      "Certifications\n",
      "----------------------------------------\n",
      "--\t4-course graduate-level certificate in Data Science, Harvard University \tJanuary 20XX\n",
      "----------------------------------------\n",
      "--\tITIL Foundation V3 \tJanuary 20XX\n",
      "----------------------------------------\n",
      "--\tProject Management Professional (PMP)® \tMarch 20XX\n",
      "----------------------------------------\n",
      "--\tCertified Salesforce Developer \tOctober 20XX\n",
      "----------------------------------------\n",
      "\n",
      "================ SEMANTIC SECTIONING OUTPUT ================\n",
      "\n",
      "\n",
      "### EDUCATION ###\n",
      "Harvard University, Extension School\n",
      "Master of Liberal Arts, Information Management Systems GPA 4.0\n",
      "--\tClass Marshall Award\n",
      "--\tDean’s List Academic Achievement Award\n",
      "--\tData Science Project: Financial Market Analysis Using Machine Learning\n",
      "--\tCapstone Project: Enterprise Data Lake\n",
      "University of Malaya\n",
      "Bachelor of Computer Science\n",
      "\n",
      "============================================================\n",
      "\n",
      "### TECHNICAL SKILLS ###\n",
      "May 20XX\n",
      "June 20XX\n",
      "• Machine Learning \t• Python/Scikit-learn\n",
      "• Spark\n",
      "• Data Visualization\n",
      "• Quantitative Analysis • Cloud Computing\n",
      "• Hadoop\n",
      "• Java/C#\n",
      "• Unix Scripting \t• Oracle/SQL Server\n",
      "• PLSQL/T-SQL\n",
      "• Data Warehouse/ETL\n",
      "• RDBMS Tuning \t• Network Protocols\n",
      "\n",
      "============================================================\n",
      "\n",
      "### PROFESSION ###\n",
      "• Agile & DevOps\n",
      "\n",
      "============================================================\n",
      "\n",
      "### AL EXPERIENCE ###\n",
      "• Web Development\n",
      "Rande Corporate & Investment Banking \tDetroit, MI\n",
      "Associate – Information Technology \tSeptember 20XX – Present\n",
      "--\tLead a team of six people to manage, operate, and support low latency posttrade brokerage platform\n",
      "--\tImproved the performance of straight-through processing by tuning database applications\n",
      "--\tReduced number of major incidents by 23% through problem management\n",
      "--\tAutomate manual back-office processing through scripting and automation engine\n",
      "--\tActively participate in and contribute to internal data science project initiatives\n",
      "Olson Financial \tSingapore\n",
      "Associate – Information Technology                                           February 20XX – September 20XX\n",
      "--\tBuilt a new application support team of five people focusing on post-trading straight- through processing and data warehouse extract-transform-load processing\n",
      "--\tDesigned and implemented global application monitoring platform.\n",
      "--\tEliminated 80% of manual checks for trading support, and decreased SLA breaches for client reporting by 15%\n",
      "Sample Resume (page 2)\n",
      "PS Engineering Information Ltd. \tSingapore\n",
      "Software Developer – Technology Office \tJuly 20XX – January 20XX\n",
      "--\tBuilt Command & Control System for Singapore Civil Defence Force using C# .NET WCF Services\n",
      "--\tIntegrated proprietary software components with commercial off-the-shell software product\n",
      "Well \tBeijing, China\n",
      "Software Developer \tJune 20XX – June 20XX\n",
      "--\tBuilt supply chain management system using Java Spring/Hibernate Framework and Service Oriented Architecture\n",
      "--\tImproved the performance of real-time business activity monitoring report and reduce the report response time by more than 50%\n",
      "Silver Technologies Ltd. \tSingapore\n",
      "Software Developer \tMay 20XX – May 20XX\n",
      "--\tDeveloped web-based Point of Sale (POS) application using C# .NET for a multinational fashion retailor\n",
      "--\tResearched and implemented RFID authentication software module\n",
      "\n",
      "============================================================\n",
      "\n",
      "### CERTIFICATIONS ###\n",
      "--\t4-course graduate-level certificate in Data Science, Harvard University \tJanuary 20XX\n",
      "--\tITIL Foundation V3 \tJanuary 20XX\n",
      "--\tProject Management Professional (PMP)® \tMarch 20XX\n",
      "--\tCertified Salesforce Developer \tOctober 20XX\n",
      "\n",
      "============================================================\n",
      "\n",
      "================ NOUN CHUNKS PER SECTION ================\n",
      "\n",
      "\n",
      "### EDUCATION ###\n",
      "harvard university, extension school master, liberal arts, information management systems gpa, --\tclass marshall award, dean, list academic achievement award, data science project, financial market analysis, machine learning, capstone project, malaya, computer science\n",
      "============================================================\n",
      "\n",
      "### TECHNICAL SKILLS ###\n",
      "may 20xx june 20xx •, machine learning, data visualization, quantitative analysis • cloud computing • hadoop • java/c#, unix scripting \t• oracle/sql server, • plsql/t-sql • data warehouse/etl, • rdbms tuning \t• network protocols\n",
      "============================================================\n",
      "\n",
      "### PROFESSION ###\n",
      "\n",
      "============================================================\n",
      "\n",
      "### AL EXPERIENCE ###\n",
      "• web development rande corporate & investment banking \tdetroit, mi associate, information technology, september, a team, six people, low latency posttrade brokerage platform, the performance, straight-through processing, database applications, reduced number, major incidents, 23%, problem management, automate manual back-office processing, scripting, automation engine, internal data science project initiatives, olson financial \tsingapore associate, a new application support team, five people, post-trading straight-, processing and data warehouse extract-transform-load processing, global application monitoring platform, 80%, manual checks, trading support, sla breaches, client reporting, 15% sample resume, page, ps engineering information ltd. \tsingapore software developer, technology office, january, singapore civil defence force, c#, wcf services, integrated proprietary software components, the-shell, beijing, june, built supply chain management system, java spring/hibernate framework, service oriented architecture, real-time business activity monitoring report, the report response time, more than 50%, silver technologies ltd. \tsingapore software developer, sale, .net, a multinational fashion retailor, rfid authentication software module\n",
      "============================================================\n",
      "\n",
      "### CERTIFICATIONS ###\n",
      "--\t4-course graduate-level certificate, data science, harvard university, itil foundation v3, project management professional (pmp)®, certified salesforce developer \toctober\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import pymupdf4llm\n",
    "from docx2python import docx2python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "\n",
    "# ================================\n",
    "# UNIVERSAL TEXT EXTRACTION\n",
    "# ================================\n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_pdf_blocks(file_path)\n",
    "    elif ext in [\".docx\", \".doc\"]:\n",
    "        # Use robust docx2python block extraction for section-aware parsing\n",
    "        return extract_docx_sections(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Use PDF or DOCX.\")\n",
    "\n",
    "# ================================\n",
    "# IMPROVED DOCX SECTION EXTRACTION\n",
    "# ================================\n",
    "def extract_docx_sections(filepath):\n",
    "    doc_result = docx2python(filepath)\n",
    "    main_content = doc_result.body\n",
    "    sections = []\n",
    "    section_heading = None\n",
    "    section_content = []\n",
    "    section_blocks = []\n",
    "\n",
    "    for outer in main_content:\n",
    "        for row in outer:\n",
    "            for cell in row:\n",
    "                for para in cell:\n",
    "                    para_str = para.strip()\n",
    "                    # Heuristic: treat all-uppercase non-trivial lines as section headings\n",
    "                    if para_str.isupper() and len(para_str) > 2: \n",
    "                        if section_heading or section_content:\n",
    "                            section_blocks.append((section_heading, '\\n'.join(section_content).strip()))\n",
    "                            section_content = []\n",
    "                        section_heading = para_str\n",
    "                    else:\n",
    "                        if para_str:\n",
    "                            section_content.append(para_str)\n",
    "    if section_heading or section_content:\n",
    "        section_blocks.append((section_heading, '\\n'.join(section_content).strip()))\n",
    "\n",
    "    # Compose raw blocks and also a single flattened list for semantic sectioning\n",
    "    all_text = []\n",
    "    for heading, content in section_blocks:\n",
    "        if heading:\n",
    "            all_text.append(heading)\n",
    "        if content:\n",
    "            all_text.extend(content.splitlines())\n",
    "\n",
    "    return all_text\n",
    "\n",
    "# ================================\n",
    "# PDF PARSING (COLUMN-AWARE & MERGED)\n",
    "# ================================\n",
    "def extract_column_aware_blocks(pdf_path, column_gap=50):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_blocks = []\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"blocks\", sort=True)\n",
    "        blocks = sorted(blocks, key=lambda b: (b[0], b[1]))\n",
    "        left_col, right_col = [], []\n",
    "        if blocks:\n",
    "            page_width = page.rect.width\n",
    "            center_line = page_width / 2\n",
    "            for b in blocks:\n",
    "                x0, y0, x1, y1, text, *_ = b\n",
    "                if x1 < center_line - column_gap:\n",
    "                    left_col.append((y0, text.strip()))\n",
    "                else:\n",
    "                    right_col.append((y0, text.strip()))\n",
    "            left_col_sorted = [t for _, t in sorted(left_col)]\n",
    "            right_col_sorted = [t for _, t in sorted(right_col)]\n",
    "            combined = right_col_sorted + left_col_sorted\n",
    "            all_blocks.extend([t for t in combined if t])\n",
    "    doc.close()\n",
    "    return all_blocks\n",
    "\n",
    "def extract_pdf_blocks(pdf_path):\n",
    "    blocks_code1 = extract_column_aware_blocks(pdf_path)\n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "    blocks_code2 = [\n",
    "        block.strip()\n",
    "        for block in md_text.split(\"\\n\\n\")\n",
    "        if block.strip() and len(block.strip()) > 5\n",
    "    ]\n",
    "    md_chunks = pymupdf4llm.to_markdown(pdf_path, page_chunks=True)\n",
    "    page_chunk_blocks = [\n",
    "        chunk[\"text\"].strip()\n",
    "        for chunk in md_chunks\n",
    "        if isinstance(chunk, dict) and \"text\" in chunk and chunk[\"text\"].strip()\n",
    "    ]\n",
    "\n",
    "    def jaccard_similarity(s1, s2, threshold=0.7):\n",
    "        set1, set2 = set(s1.lower().strip()), set(s2.lower().strip())\n",
    "        intersection = set1 & set2\n",
    "        union = set1 | set2\n",
    "        if not union:\n",
    "            return False\n",
    "        return len(intersection) / len(union) > threshold\n",
    "\n",
    "    missing_from_code2 = []\n",
    "    for block1 in blocks_code1:\n",
    "        if not any(jaccard_similarity(block1, block2) for block2 in blocks_code2):\n",
    "            missing_from_code2.append(block1)\n",
    "    final_blocks = blocks_code2 + missing_from_code2\n",
    "    return final_blocks, page_chunk_blocks\n",
    "\n",
    "# ================================\n",
    "# SEMANTIC HEADING DETECTION + SECTION GROUPING\n",
    "# ================================\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "canonical_headings = [\n",
    "    \"experience\", \"work experience\", \"education\", \"academic history\",\n",
    "    \"projects\", \"technical projects\", \"skills\", \"key skills\",\n",
    "    \"certifications\", \"achievements\", \"publications\", \"personal details\", \"contact\",\n",
    "    \"technical skills\", \"professional experience\"\n",
    "]\n",
    "canonical_embeddings = model.encode(canonical_headings, convert_to_tensor=True)\n",
    "\n",
    "def semantic_heading_detection(lines, threshold=0.55):\n",
    "    headings = []\n",
    "    for line in lines:\n",
    "        line_clean = line.strip().lower()\n",
    "        if not line_clean or len(line_clean) < 3:\n",
    "            continue\n",
    "        emb = model.encode(line_clean, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(emb, canonical_embeddings)[0]\n",
    "        max_score = float(cosine_scores.max())\n",
    "        if max_score >= threshold:\n",
    "            headings.append(line.strip())\n",
    "    return headings\n",
    "\n",
    "def semantic_sectioning(lines, threshold=0.55):\n",
    "    detected_headings = semantic_heading_detection(lines, threshold)\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip() in detected_headings:\n",
    "            if current_section and buffer:\n",
    "                sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "            current_section = line.strip()\n",
    "            buffer = []\n",
    "        else:\n",
    "            buffer.append(line.strip())\n",
    "\n",
    "    if current_section and buffer:\n",
    "        sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "    return sections\n",
    "\n",
    "# ================================\n",
    "# ENHANCED NOUN CHUNK EXTRACTION\n",
    "# ================================\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def filter_and_clean_noun_chunks(doc):\n",
    "    seen = set()\n",
    "    clean_chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        text = chunk.text.strip().lower()\n",
    "        if not text or len(text) < 2:\n",
    "            continue\n",
    "        if all(token.is_stop for token in chunk):\n",
    "            continue\n",
    "        if text in seen:\n",
    "            continue\n",
    "        seen.add(text)\n",
    "        clean_chunks.append(text)\n",
    "    return clean_chunks\n",
    "\n",
    "def extract_section_noun_chunks(sections):\n",
    "    section_chunks = {}\n",
    "    for heading, text in sections.items():\n",
    "        flat = \" \".join(line.strip() for line in text.splitlines() if line.strip())\n",
    "        doc = nlp(flat)\n",
    "        noun_chunks = filter_and_clean_noun_chunks(doc)\n",
    "        section_chunks[heading] = noun_chunks\n",
    "    return section_chunks\n",
    "\n",
    "# ================================\n",
    "# DRIVER\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"C:\\\\Users\\\\zenit\\\\Downloads\\\\1 word resume.docx\"\n",
    "    extracted_output = extract_text(file_path)\n",
    "\n",
    "    if isinstance(extracted_output, tuple):\n",
    "        final_blocks, page_chunks = extracted_output\n",
    "    else:\n",
    "        final_blocks = extracted_output\n",
    "        page_chunks = []\n",
    "\n",
    "    print(f\"\\nParsed Output from: {os.path.basename(file_path)}\")\n",
    "    print(\"\\n================ RAW PARSED TEXT BLOCKS ================\\n\")\n",
    "    for block in final_blocks:\n",
    "        print(block)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n================ SEMANTIC SECTIONING OUTPUT ================\\n\")\n",
    "    sections = semantic_sectioning(final_blocks)\n",
    "\n",
    "    for heading, content in sections.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\\n{content}\\n\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n================ NOUN CHUNKS PER SECTION ================\\n\")\n",
    "    section_chunks = extract_section_noun_chunks(sections)\n",
    "    for heading, chunks in section_chunks.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\")\n",
    "        print(\", \".join(chunks))\n",
    "        print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "801a8f79-c8d5-473f-968e-2f0860082985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed Output from: 9 word resume.docx\n",
      "\n",
      "================ RAW PARSED TEXT BLOCKS ================\n",
      "\n",
      "Sample Resume\n",
      "----------------------------------------\n",
      "Georgina Santiago\n",
      "----------------------------------------\n",
      "Cambridge, MA / 555-555-5555 / you@gmail.com / www.linkedin.com/in/profile\n",
      "----------------------------------------\n",
      "EDUCATION\n",
      "----------------------------------------\n",
      "Harvard University Extension School\n",
      "----------------------------------------\n",
      "Cambridge, MA\n",
      "----------------------------------------\n",
      "Bachelor of Liberal Arts, Field of Study Economics\n",
      "----------------------------------------\n",
      "Cum Laude, Dean’s List, GPA 3.62\n",
      "----------------------------------------\n",
      "Worked up to 40+ hours a week to defray cost of tuition\n",
      "----------------------------------------\n",
      "May 2016\n",
      "----------------------------------------\n",
      "EXPERIENCE\n",
      "----------------------------------------\n",
      "Hangtime Wholesale Wine Company \tBoston, MA\n",
      "----------------------------------------\n",
      "Sales Representative \t20XX-Present\n",
      "----------------------------------------\n",
      "Opened and maintain 40 accounts in the greater Boston area. Conduct in-store tastings and staff trainings to generate greater revenue. Create and distribute promotional materials.\n",
      "----------------------------------------\n",
      "Christie’s Auction House \tNew York, NY\n",
      "----------------------------------------\n",
      "Intern, Fine and Rare Wine Department \t20XX\n",
      "----------------------------------------\n",
      "Performed pre-and post-sale statistical analysis. Researched and executed mass mailing in order to generate new consignments. Researched potential domestic clients for annual Hospice de Beaune Auction. Generated contracts for consignors. Served as front-line contact for both existing clients and potential consignors, handling incoming and outgoing correspondence. Compiled and entered tasting notes for auction catalogue.\n",
      "----------------------------------------\n",
      "Montagna Bar and Restaurant \tAspen, CO Back-Server, Cocktail Server, Food-Runner \t20XX\n",
      "----------------------------------------\n",
      "Active participant in wine program, including weekly blind-tastings. Created suitable beverage pairing for patrons.\n",
      "----------------------------------------\n",
      "Shay’s Pub and Wine Bar \tCambridge, MA\n",
      "----------------------------------------\n",
      "Server, Bartender, Floor Manager \t20XX-20XX\n",
      "----------------------------------------\n",
      "Coordinated and promoted weekly specials to generate optimal revenue. Participated in development, expansion and improvement of wine program. Recruited and trained all floor staff. Increased overall restaurant sales by 75%.\n",
      "----------------------------------------\n",
      "The Second Glass \tBoston, MA\n",
      "----------------------------------------\n",
      "Staff Writer \t20XX-20XX\n",
      "----------------------------------------\n",
      "Launched premier issue of print and online wine magazine. Increased public visibility through participation in wine related events. Provided up to three articles per print issue and once weekly for online issue. Conducted research and interviews for articles.\n",
      "----------------------------------------\n",
      "Certifications: Court of Master Sommeliers: Introductory Course\n",
      "----------------------------------------\n",
      "WSET Level 3 Advanced Certificate in Wine and Spirits (Pass with Merit)\n",
      "----------------------------------------\n",
      "Paris Chamber of Commerce and Industry Diploma in Business French Member, Boston Sommelier Society\n",
      "----------------------------------------\n",
      "Volunteer: \t Domaine Carrett Bully, France 20XX: Vineyard and Cellar\n",
      "----------------------------------------\n",
      "Management Ovid Vineyards, St Helena, California 20XX: Office and Events Support\n",
      "----------------------------------------\n",
      "\n",
      "================ SEMANTIC SECTIONING OUTPUT ================\n",
      "\n",
      "\n",
      "### EDUCATION ###\n",
      "Harvard University Extension School\n",
      "Cambridge, MA\n",
      "Bachelor of Liberal Arts, Field of Study Economics\n",
      "Cum Laude, Dean’s List, GPA 3.62\n",
      "Worked up to 40+ hours a week to defray cost of tuition\n",
      "May 2016\n",
      "\n",
      "============================================================\n",
      "\n",
      "### EXPERIENCE ###\n",
      "Hangtime Wholesale Wine Company \tBoston, MA\n",
      "Sales Representative \t20XX-Present\n",
      "Opened and maintain 40 accounts in the greater Boston area. Conduct in-store tastings and staff trainings to generate greater revenue. Create and distribute promotional materials.\n",
      "Christie’s Auction House \tNew York, NY\n",
      "Intern, Fine and Rare Wine Department \t20XX\n",
      "Performed pre-and post-sale statistical analysis. Researched and executed mass mailing in order to generate new consignments. Researched potential domestic clients for annual Hospice de Beaune Auction. Generated contracts for consignors. Served as front-line contact for both existing clients and potential consignors, handling incoming and outgoing correspondence. Compiled and entered tasting notes for auction catalogue.\n",
      "Montagna Bar and Restaurant \tAspen, CO Back-Server, Cocktail Server, Food-Runner \t20XX\n",
      "Active participant in wine program, including weekly blind-tastings. Created suitable beverage pairing for patrons.\n",
      "Shay’s Pub and Wine Bar \tCambridge, MA\n",
      "Server, Bartender, Floor Manager \t20XX-20XX\n",
      "Coordinated and promoted weekly specials to generate optimal revenue. Participated in development, expansion and improvement of wine program. Recruited and trained all floor staff. Increased overall restaurant sales by 75%.\n",
      "The Second Glass \tBoston, MA\n",
      "Staff Writer \t20XX-20XX\n",
      "Launched premier issue of print and online wine magazine. Increased public visibility through participation in wine related events. Provided up to three articles per print issue and once weekly for online issue. Conducted research and interviews for articles.\n",
      "\n",
      "============================================================\n",
      "\n",
      "### CERTIFICATIONS: COURT OF MASTER SOMMELIERS: INTRODUCTORY COURSE ###\n",
      "WSET Level 3 Advanced Certificate in Wine and Spirits (Pass with Merit)\n",
      "Paris Chamber of Commerce and Industry Diploma in Business French Member, Boston Sommelier Society\n",
      "Volunteer: \t Domaine Carrett Bully, France 20XX: Vineyard and Cellar\n",
      "Management Ovid Vineyards, St Helena, California 20XX: Office and Events Support\n",
      "\n",
      "============================================================\n",
      "\n",
      "================ SPACY FEATURES PER SECTION ================\n",
      "\n",
      "\n",
      "### EDUCATION ###\n",
      "Nouns: cost, hours, list, tuition, week\n",
      "Noun Chunks: cost, dean’s list, field, gpa, harvard university extension school cambridge, liberal arts, ma bachelor, study economics cum laude, tuition\n",
      "Compound Nouns: harvard university extension school cambridge, study economics cum laude\n",
      "Verbs: defray, work\n",
      "Verbal Nouns: \n",
      "============================================================\n",
      "\n",
      "### EXPERIENCE ###\n",
      "Nouns: %, 20xx, accounts, analysis, area, articles, auction, beverage, catalogue, clients, conduct, consignments, consignors, contact, contracts, correspondence, development, events, expansion, floor, improvement, interviews, issue, line, magazine, mailing, mass, materials, notes, order, participant, participation, patrons, present, print, program, research, restaurant, revenue, sales, specials, staff, store, tasting, tastings, trainings, visibility, wine\n",
      "Noun Chunks: 40 accounts, 75%, active participant, all floor staff, annual hospice de beaune auction, articles, auction catalogue, bartender, both existing clients, christie’s auction house \tnew york, co back-server, cocktail server, conduct, conducted research, consignors, development, expansion, fine, floor manager \t20xx-20xx, food-runner \t20xx, front-line contact, generated contracts, greater revenue, hangtime wholesale wine company \tboston, improvement, incoming and outgoing correspondence, increased overall restaurant sales, increased public visibility, interviews, launched premier issue, ma server, ma staff writer \t20xx-20xx, mass mailing, montagna bar, new consignments, ny intern, online issue, online wine magazine, optimal revenue, order, participation, patrons, potential consignors, potential domestic clients, pre-and post-sale statistical analysis, present, print, print issue, promotional materials, rare wine department \t20xx, restaurant \taspen, shay’s pub, staff trainings, store, tasting notes, the greater boston area, the second glass \tboston, up to three articles, weekly blind-tastings, weekly specials, wine bar \tcambridge, wine program, wine related events\n",
      "Compound Nouns: all floor staff, annual hospice de beaune auction, auction catalogue, christie’s auction house \tnew york, co back-server, cocktail server, floor manager \t20xx-20xx, food-runner \t20xx, front-line contact, hangtime wholesale wine company \tboston, increased overall restaurant sales, launched premier issue, ma staff writer \t20xx-20xx, mass mailing, online wine magazine, print issue, rare wine department \t20xx, staff trainings, tasting notes, the greater boston area, the second glass \tboston, wine bar \tcambridge, wine program, wine related events\n",
      "Verbs: compile, conduct, create, distribute, enter, execute, exist, generate, handle, include, increase, launch, maintain, open, pair, participate, perform, promote, provide, recruit, relate, research, serve, train\n",
      "Verbal Nouns: conduct, existing, handling, including, notes, pairing, tasting, tastings\n",
      "============================================================\n",
      "\n",
      "### CERTIFICATIONS: COURT OF MASTER SOMMELIERS: INTRODUCTORY COURSE ###\n",
      "Nouns: support\n",
      "Noun Chunks: boston sommelier society volunteer, business french member, california, commerce, domaine carrett bully, merit, office and events support, spirits, st helena, vineyard and cellar management ovid vineyards, wine, wset level 3 advanced certificate\n",
      "Compound Nouns: boston sommelier society volunteer, domaine carrett bully, vineyard and cellar management ovid vineyards, wset level 3 advanced certificate\n",
      "Verbs: pass\n",
      "Verbal Nouns: \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pymupdf4llm\n",
    "from docx2python import docx2python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "\n",
    "# ================================\n",
    "# UNIVERSAL TEXT EXTRACTION\n",
    "# ================================\n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_pdf_blocks(file_path)\n",
    "    elif ext in [\".docx\", \".doc\"]:\n",
    "        # Return exact layout: line-by-line, order preserved, no grouping\n",
    "        return extract_docx_exact_layout(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Use PDF or DOCX.\")\n",
    "\n",
    "# ================================\n",
    "# DOCX LINE-BY-LINE ORDER-PRESERVING EXTRACTION\n",
    "# ================================\n",
    "def extract_docx_exact_layout(filepath):\n",
    "    doc_result = docx2python(filepath)\n",
    "    main_content = doc_result.body\n",
    "    all_text = []\n",
    "    for section in main_content:\n",
    "        for row in section:\n",
    "            for cell in row:\n",
    "                for para in cell:\n",
    "                    para_str = para.strip()\n",
    "                    if para_str:\n",
    "                        all_text.append(para_str)\n",
    "    return all_text\n",
    "\n",
    "# ================================\n",
    "# PDF PARSING (COLUMN-AWARE & MERGED)\n",
    "# ================================\n",
    "def extract_column_aware_blocks(pdf_path, column_gap=50):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_blocks = []\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"blocks\", sort=True)\n",
    "        blocks = sorted(blocks, key=lambda b: (b[0], b[1]))\n",
    "        left_col, right_col = [], []\n",
    "        if blocks:\n",
    "            page_width = page.rect.width\n",
    "            center_line = page_width / 2\n",
    "            for b in blocks:\n",
    "                x0, y0, x1, y1, text, *_ = b\n",
    "                if x1 < center_line - column_gap:\n",
    "                    left_col.append((y0, text.strip()))\n",
    "                else:\n",
    "                    right_col.append((y0, text.strip()))\n",
    "            left_col_sorted = [t for _, t in sorted(left_col)]\n",
    "            right_col_sorted = [t for _, t in sorted(right_col)]\n",
    "            combined = right_col_sorted + left_col_sorted\n",
    "            all_blocks.extend([t for t in combined if t])\n",
    "    doc.close()\n",
    "    return all_blocks\n",
    "\n",
    "def extract_pdf_blocks(pdf_path):\n",
    "    blocks_code1 = extract_column_aware_blocks(pdf_path)\n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "    blocks_code2 = [\n",
    "        block.strip()\n",
    "        for block in md_text.split(\"\\n\\n\")\n",
    "        if block.strip() and len(block.strip()) > 5\n",
    "    ]\n",
    "    md_chunks = pymupdf4llm.to_markdown(pdf_path, page_chunks=True)\n",
    "    page_chunk_blocks = [\n",
    "        chunk[\"text\"].strip()\n",
    "        for chunk in md_chunks\n",
    "        if isinstance(chunk, dict) and \"text\" in chunk and chunk[\"text\"].strip()\n",
    "    ]\n",
    "\n",
    "    def jaccard_similarity(s1, s2, threshold=0.7):\n",
    "        set1, set2 = set(s1.lower().strip()), set(s2.lower().strip())\n",
    "        intersection = set1 & set2\n",
    "        union = set1 | set2\n",
    "        if not union:\n",
    "            return False\n",
    "        return len(intersection) / len(union) > threshold\n",
    "\n",
    "    missing_from_code2 = []\n",
    "    for block1 in blocks_code1:\n",
    "        if not any(jaccard_similarity(block1, block2) for block2 in blocks_code2):\n",
    "            missing_from_code2.append(block1)\n",
    "    final_blocks = blocks_code2 + missing_from_code2\n",
    "    return final_blocks, page_chunk_blocks\n",
    "\n",
    "# ================================\n",
    "# SEMANTIC HEADING DETECTION + SECTION GROUPING\n",
    "# ================================\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "canonical_headings = [\n",
    "    \"experience\", \"work experience\", \"education\", \"academic history\",\n",
    "    \"projects\", \"technical projects\", \"skills\", \"key skills\",\n",
    "    \"certifications\", \"achievements\", \"publications\", \"personal details\", \"contact\",\n",
    "    \"technical skills\", \"professional experience\",\"core competencies\"\n",
    "]\n",
    "canonical_embeddings = model.encode(canonical_headings, convert_to_tensor=True)\n",
    "\n",
    "def semantic_heading_detection(lines, threshold=0.6):\n",
    "    headings = []\n",
    "    for line in lines:\n",
    "        line_clean = line.strip().lower()\n",
    "        if not line_clean or len(line_clean) < 3:\n",
    "            continue\n",
    "        emb = model.encode(line_clean, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(emb, canonical_embeddings)[0]\n",
    "        max_score = float(cosine_scores.max())\n",
    "        if max_score >= threshold:\n",
    "            headings.append(line.strip())\n",
    "    return headings\n",
    "\n",
    "def semantic_sectioning(lines, threshold=0.55):\n",
    "    detected_headings = semantic_heading_detection(lines, threshold)\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip() in detected_headings:\n",
    "            if current_section and buffer:\n",
    "                sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "            current_section = line.strip()\n",
    "            buffer = []\n",
    "        else:\n",
    "            buffer.append(line.strip())\n",
    "\n",
    "    if current_section and buffer:\n",
    "        sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "    return sections\n",
    "\n",
    "# ================================\n",
    "# ENHANCED NOUN CHUNK EXTRACTION\n",
    "# ================================\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def extract_features_per_section(sections):\n",
    "    section_features = {}\n",
    "    for heading, text in sections.items():\n",
    "        # Flatten and clean section text\n",
    "        flat = \" \".join(line.strip() for line in text.splitlines() if line.strip())\n",
    "        doc = nlp(flat)\n",
    "        \n",
    "        # spaCy features\n",
    "        nouns = set(token.text.strip().lower() for token in doc if token.pos_ == \"NOUN\")\n",
    "        noun_chunks = set(chunk.text.strip().lower() for chunk in doc.noun_chunks)\n",
    "        compound_nouns = set(\n",
    "            chunk.text.strip().lower()\n",
    "            for chunk in doc.noun_chunks\n",
    "            if sum(t.pos_ == \"NOUN\" or t.dep_ == \"compound\" for t in chunk) > 1\n",
    "        )\n",
    "        verbs = set(token.lemma_.strip().lower() for token in doc if token.pos_ == \"VERB\")\n",
    "        verbal_nouns = set(\n",
    "            token.text.strip().lower()\n",
    "            for token in doc\n",
    "            if token.tag_ == \"VBG\" or (\n",
    "                token.pos_ == \"NOUN\" and nlp(token.lemma_)[0].pos_ == \"VERB\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        section_features[heading] = {\n",
    "            \"nouns\": sorted(nouns),\n",
    "            \"noun_chunks\": sorted(noun_chunks),\n",
    "            \"compound_nouns\": sorted(compound_nouns),\n",
    "            \"verbs\": sorted(verbs),\n",
    "            \"verbal_nouns\": sorted(verbal_nouns)\n",
    "        }\n",
    "    return section_features\n",
    "\n",
    "# ================================\n",
    "# DRIVER\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"C:\\\\Users\\\\zenit\\\\Downloads\\\\9 word resume.docx\"\n",
    "    extracted_output = extract_text(file_path)\n",
    "\n",
    "    if isinstance(extracted_output, tuple):\n",
    "        final_blocks, page_chunks = extracted_output\n",
    "    else:\n",
    "        final_blocks = extracted_output\n",
    "        page_chunks = []\n",
    "\n",
    "    print(f\"\\nParsed Output from: {os.path.basename(file_path)}\")\n",
    "    print(\"\\n================ RAW PARSED TEXT BLOCKS ================\\n\")\n",
    "    for block in final_blocks:\n",
    "        print(block)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n================ SEMANTIC SECTIONING OUTPUT ================\\n\")\n",
    "    sections = semantic_sectioning(final_blocks)\n",
    "    for heading, content in sections.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\\n{content}\\n\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n================ SPACY FEATURES PER SECTION ================\\n\")\n",
    "    section_features = extract_features_per_section(sections)\n",
    "    for heading, feats in section_features.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\")\n",
    "        print(\"Nouns: \" + \", \".join(feats[\"nouns\"]))\n",
    "        print(\"Noun Chunks: \" + \", \".join(feats[\"noun_chunks\"]))\n",
    "        print(\"Compound Nouns: \" + \", \".join(feats[\"compound_nouns\"]))\n",
    "        print(\"Verbs: \" + \", \".join(feats[\"verbs\"]))\n",
    "        print(\"Verbal Nouns: \" + \", \".join(feats[\"verbal_nouns\"]))\n",
    "        print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b6df8-ea51-4233-b7dd-269f9d42e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm verify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c1120b4-ef4e-4ae2-945a-c80454b0ab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed Output from: 9 word resume.pdf\n",
      "\n",
      "================ RAW PARSED TEXT BLOCKS ================\n",
      "\n",
      "# Sample Resume\n",
      "----------------------------------------\n",
      "### **Georgina Santiago**\n",
      "----------------------------------------\n",
      "Cambridge, MA / 555-555-5555 / you@gmail.com / www.linkedin.com/in/profile\n",
      "----------------------------------------\n",
      "**EDUCATION**\n",
      "**Harvard University Extension School** Cambridge, MA\n",
      "Bachelor of Liberal Arts, Field of Study Economics May 2016\n",
      "Cum Laude, Dean’s List, GPA 3.62\n",
      "Worked up to 40+ hours a week to defray cost of tuition\n",
      "----------------------------------------\n",
      "**EXPERIENCE**\n",
      "----------------------------------------\n",
      "**Hangtime Wholesale Wine Company** Boston, MA\n",
      "**Sales Representative** 20XX-Present\n",
      "Opened and maintain 40 accounts in the greater Boston area. Conduct in-store tastings and staff\n",
      "trainings to generate greater revenue. Create and distribute promotional materials.\n",
      "----------------------------------------\n",
      "**Christie’s Auction House** New York, NY\n",
      "**Intern, Fine and Rare Wine Department** 20XX\n",
      "Performed pre-and post-sale statistical analysis. Researched and executed mass mailing in order\n",
      "to generate new consignments. Researched potential domestic clients for annual Hospice de\n",
      "Beaune Auction. Generated contracts for consignors. Served as front-line contact for both\n",
      "existing clients and potential consignors, handling incoming and outgoing correspondence.\n",
      "Compiled and entered tasting notes for auction catalogue.\n",
      "----------------------------------------\n",
      "**Montagna Bar and Restaurant** Aspen, CO\n",
      "**Back-Server, Cocktail Server, Food-Runner** 20XX\n",
      "Active participant in wine program, including weekly blind-tastings. Created suitable beverage\n",
      "pairing for patrons.\n",
      "----------------------------------------\n",
      "**Shay’s Pub and Wine Bar** Cambridge, MA\n",
      "**Server, Bartender, Floor Manager** 20XX-20XX\n",
      "Coordinated and promoted weekly specials to generate optimal revenue. Participated in\n",
      "development, expansion and improvement of wine program. Recruited and trained all floor\n",
      "staff. Increased overall restaurant sales by 75%.\n",
      "----------------------------------------\n",
      "**The Second Glass** Boston, MA\n",
      "**Staff Writer** 20XX-20XX\n",
      "Launched premier issue of print and online wine magazine. Increased public visibility through\n",
      "participation in wine related events. Provided up to three articles per print issue and once weekly\n",
      "for online issue. Conducted research and interviews for articles.\n",
      "----------------------------------------\n",
      "**Certifications:** Court of Master Sommeliers: Introductory Course\n",
      "----------------------------------------\n",
      "WSET Level 3 Advanced Certificate in Wine and Spirits (Pass with Merit)\n",
      "Paris Chamber of Commerce and Industry Diploma in Business French\n",
      "Member, Boston Sommelier Society\n",
      "----------------------------------------\n",
      "**Volunteer:** Domaine Carrett Bully, France 20XX: Vineyard and Cellar\n",
      "Management Ovid Vineyards, St Helena, California 20XX: Office and\n",
      "Events Support\n",
      "----------------------------------------\n",
      "May 2016\n",
      "----------------------------------------\n",
      "\n",
      "================ SEMANTIC SECTIONING OUTPUT ================\n",
      "\n",
      "\n",
      "### **EXPERIENCE** ###\n",
      "**Hangtime Wholesale Wine Company** Boston, MA\n",
      "**Sales Representative** 20XX-Present\n",
      "Opened and maintain 40 accounts in the greater Boston area. Conduct in-store tastings and staff\n",
      "trainings to generate greater revenue. Create and distribute promotional materials.\n",
      "**Christie’s Auction House** New York, NY\n",
      "**Intern, Fine and Rare Wine Department** 20XX\n",
      "Performed pre-and post-sale statistical analysis. Researched and executed mass mailing in order\n",
      "to generate new consignments. Researched potential domestic clients for annual Hospice de\n",
      "Beaune Auction. Generated contracts for consignors. Served as front-line contact for both\n",
      "existing clients and potential consignors, handling incoming and outgoing correspondence.\n",
      "Compiled and entered tasting notes for auction catalogue.\n",
      "**Montagna Bar and Restaurant** Aspen, CO\n",
      "**Back-Server, Cocktail Server, Food-Runner** 20XX\n",
      "Active participant in wine program, including weekly blind-tastings. Created suitable beverage\n",
      "pairing for patrons.\n",
      "**Shay’s Pub and Wine Bar** Cambridge, MA\n",
      "**Server, Bartender, Floor Manager** 20XX-20XX\n",
      "Coordinated and promoted weekly specials to generate optimal revenue. Participated in\n",
      "development, expansion and improvement of wine program. Recruited and trained all floor\n",
      "staff. Increased overall restaurant sales by 75%.\n",
      "**The Second Glass** Boston, MA\n",
      "**Staff Writer** 20XX-20XX\n",
      "Launched premier issue of print and online wine magazine. Increased public visibility through\n",
      "participation in wine related events. Provided up to three articles per print issue and once weekly\n",
      "for online issue. Conducted research and interviews for articles.\n",
      "\n",
      "============================================================\n",
      "\n",
      "### **CERTIFICATIONS:** COURT OF MASTER SOMMELIERS: INTRODUCTORY COURSE ###\n",
      "WSET Level 3 Advanced Certificate in Wine and Spirits (Pass with Merit)\n",
      "Paris Chamber of Commerce and Industry Diploma in Business French\n",
      "Member, Boston Sommelier Society\n",
      "**Volunteer:** Domaine Carrett Bully, France 20XX: Vineyard and Cellar\n",
      "Management Ovid Vineyards, St Helena, California 20XX: Office and\n",
      "Events Support\n",
      "May 2016\n",
      "\n",
      "============================================================\n",
      "\n",
      "================ SPA-CY FEATURES PER SECTION ================\n",
      "\n",
      "\n",
      "### **EXPERIENCE** ###\n",
      "Noun Chunks:  hangtime wholesale wine company, * boston, **sales representative, present, 40 accounts, the greater boston area, conduct, store, staff trainings, greater revenue, promotional materials, * new york, pre-and post-sale statistical analysis, mass mailing, order, new consignments, potential domestic clients, annual hospice de beaune auction, generated contracts, consignors, front-line contact, both existing clients, potential consignors, incoming and outgoing correspondence, tasting notes, auction catalogue, **montagna bar, restaurant, * aspen, co, *back-server, cocktail server, food-runner, active participant, wine program, weekly blind-tastings, patrons, **shay, pub, wine bar, * cambridge, ma, **server, bartender, floor manager, weekly specials, optimal revenue, development, expansion, improvement, all floor staff, increased overall restaurant sales, 75%, **the second glass** boston, staff writer, 20xx-20xx, print, online wine magazine, increased public visibility, participation, wine related events, up to three articles, print issue, online issue, conducted research, interviews, articles\n",
      "Compound Nouns:  hangtime wholesale wine company, **sales representative, the greater boston area, staff trainings, * new york, mass mailing, annual hospice de beaune auction, front-line contact, tasting notes, auction catalogue, **montagna bar, *back-server, cocktail server, food-runner, wine program, wine bar, floor manager, all floor staff, increased overall restaurant sales, staff writer, online wine magazine, print issue\n",
      "Verbal Nouns:  \n",
      "Verbs:  compile, conduct, create, distribute, enter, execute, exist, generate, handle, include, increase, launch, maintain, open, pair, participate, perform, promote, provide, recruit, relate, research, serve, train\n",
      "Dates:  annual, weekly\n",
      "============================================================\n",
      "\n",
      "### **CERTIFICATIONS:** COURT OF MASTER SOMMELIERS: INTRODUCTORY COURSE ###\n",
      "Noun Chunks:  wset level 3 advanced certificate, wine, spirits, merit, commerce, business french member, volunteer:** domaine carrett bully, vineyard and cellar management ovid vineyards, st helena, california\n",
      "Compound Nouns:  wset level 3 advanced certificate, business french member, volunteer:** domaine carrett bully, vineyard and cellar management ovid vineyards, st helena\n",
      "Verbal Nouns:  \n",
      "Verbs:  pass\n",
      "Dates:  May 2016\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pymupdf4llm\n",
    "from docx2python import docx2python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "\n",
    "# ================================\n",
    "# UNIVERSAL TEXT EXTRACTION\n",
    "# ================================\n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_pdf_blocks(file_path)\n",
    "    elif ext in [\".docx\", \".doc\"]:\n",
    "        # Return exact layout: line-by-line, order preserved, no grouping\n",
    "        return extract_docx_exact_layout(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Use PDF or DOCX.\")\n",
    "\n",
    "# ================================\n",
    "# DOCX LINE-BY-LINE ORDER-PRESERVING EXTRACTION\n",
    "# ================================\n",
    "def extract_docx_exact_layout(filepath):\n",
    "    doc_result = docx2python(filepath)\n",
    "    main_content = doc_result.body\n",
    "    all_text = []\n",
    "    for section in main_content:\n",
    "        for row in section:\n",
    "            for cell in row:\n",
    "                for para in cell:\n",
    "                    para_str = para.strip()\n",
    "                    if para_str:\n",
    "                        all_text.append(para_str)\n",
    "    return all_text\n",
    "\n",
    "# ================================\n",
    "# PDF PARSING (COLUMN-AWARE & MERGED)\n",
    "# ================================\n",
    "def extract_column_aware_blocks(pdf_path, column_gap=50):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_blocks = []\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"blocks\", sort=True)\n",
    "        blocks = sorted(blocks, key=lambda b: (b[0], b[1]))\n",
    "        left_col, right_col = [], []\n",
    "        if blocks:\n",
    "            page_width = page.rect.width\n",
    "            center_line = page_width / 2\n",
    "            for b in blocks:\n",
    "                x0, y0, x1, y1, text, *_ = b\n",
    "                if x1 < center_line - column_gap:\n",
    "                    left_col.append((y0, text.strip()))\n",
    "                else:\n",
    "                    right_col.append((y0, text.strip()))\n",
    "            left_col_sorted = [t for _, t in sorted(left_col)]\n",
    "            right_col_sorted = [t for _, t in sorted(right_col)]\n",
    "            combined = right_col_sorted + left_col_sorted\n",
    "            all_blocks.extend([t for t in combined if t])\n",
    "    doc.close()\n",
    "    return all_blocks\n",
    "\n",
    "def extract_pdf_blocks(pdf_path):\n",
    "    blocks_code1 = extract_column_aware_blocks(pdf_path)\n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "    blocks_code2 = [\n",
    "        block.strip()\n",
    "        for block in md_text.split(\"\\n\\n\")\n",
    "        if block.strip() and len(block.strip()) > 5\n",
    "    ]\n",
    "    md_chunks = pymupdf4llm.to_markdown(pdf_path, page_chunks=True)\n",
    "    page_chunk_blocks = [\n",
    "        chunk[\"text\"].strip()\n",
    "        for chunk in md_chunks\n",
    "        if isinstance(chunk, dict) and \"text\" in chunk and chunk[\"text\"].strip()\n",
    "    ]\n",
    "\n",
    "    def jaccard_similarity(s1, s2, threshold=0.7):\n",
    "        set1, set2 = set(s1.lower().strip()), set(s2.lower().strip())\n",
    "        intersection = set1 & set2\n",
    "        union = set1 | set2\n",
    "        if not union:\n",
    "            return False\n",
    "        return len(intersection) / len(union) > threshold\n",
    "\n",
    "    missing_from_code2 = []\n",
    "    for block1 in blocks_code1:\n",
    "        if not any(jaccard_similarity(block1, block2) for block2 in blocks_code2):\n",
    "            missing_from_code2.append(block1)\n",
    "    final_blocks = blocks_code2 + missing_from_code2\n",
    "    return final_blocks, page_chunk_blocks\n",
    "\n",
    "# ================================\n",
    "# SEMANTIC HEADING DETECTION + SECTION GROUPING\n",
    "# ================================\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "canonical_headings = [\n",
    "    \"experience\", \"work experience\", \"education\", \"academic history\",\n",
    "    \"projects\", \"technical projects\", \"skills\", \"key skills\",\n",
    "    \"certifications\", \"achievements\", \"publications\", \"personal details\", \"contact\",\n",
    "    \"technical skills\", \"professional experience\",\"Core competencies\"\n",
    "]\n",
    "canonical_embeddings = model.encode(canonical_headings, convert_to_tensor=True)\n",
    "\n",
    "def semantic_heading_detection(lines, threshold=0.6):\n",
    "    headings = []\n",
    "    for line in lines:\n",
    "        line_clean = line.strip().lower()\n",
    "        if not line_clean or len(line_clean) < 3:\n",
    "            continue\n",
    "        emb = model.encode(line_clean, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(emb, canonical_embeddings)[0]\n",
    "        max_score = float(cosine_scores.max())\n",
    "        if max_score >= threshold:\n",
    "            headings.append(line.strip())\n",
    "    return headings\n",
    "\n",
    "def semantic_sectioning(lines, threshold=0.55):\n",
    "    detected_headings = semantic_heading_detection(lines, threshold)\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip() in detected_headings:\n",
    "            if current_section and buffer:\n",
    "                sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "            current_section = line.strip()\n",
    "            buffer = []\n",
    "        else:\n",
    "            buffer.append(line.strip())\n",
    "\n",
    "    if current_section and buffer:\n",
    "        sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "    return sections\n",
    "\n",
    "# ================================\n",
    "# ENHANCED NOUN CHUNK CLEANING\n",
    "# ================================\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def filter_and_clean_noun_chunks(doc):\n",
    "    seen = set()\n",
    "    clean_chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        text = chunk.text.strip().lower()\n",
    "        if not text or len(text) < 2:\n",
    "            continue\n",
    "        if all(token.is_stop for token in chunk):\n",
    "            continue\n",
    "        if text in seen:\n",
    "            continue\n",
    "        seen.add(text)\n",
    "        clean_chunks.append(text)\n",
    "    return clean_chunks\n",
    "\n",
    "# ================================\n",
    "# VERBS, NOUN CHUNKS, COMPOUND NOUNS, GERUNDS, DATES PER SECTION\n",
    "# ================================\n",
    "def extract_section_spacy_features(sections):\n",
    "    section_features = {}\n",
    "    for heading, text in sections.items():\n",
    "        flat = \" \".join(line.strip() for line in text.splitlines() if line.strip())\n",
    "        doc = nlp(flat)\n",
    "        # Noun chunks\n",
    "        noun_chunks = filter_and_clean_noun_chunks(doc)\n",
    "        # Verbs (lemma, no stopwords, minlen 2)\n",
    "        verbs = sorted(set([t.lemma_ for t in doc if t.pos_ == \"VERB\" and not t.is_stop and len(t.lemma_) > 1]))\n",
    "        # Compound nouns (noun chunk with any compound token)\n",
    "        compounds = []\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if any(t.dep_ == \"compound\" for t in chunk):\n",
    "                compound_text = chunk.text.strip().lower()\n",
    "                if compound_text not in compounds:\n",
    "                    compounds.append(compound_text)\n",
    "        # Verbal nouns (gerunds: tag_ == VBG, pos_ == NOUN)\n",
    "        verbal_nouns = sorted(set([t.text for t in doc if t.tag_ == \"VBG\" and t.pos_ == \"NOUN\"]))\n",
    "        # Dates (NER)\n",
    "        dates = sorted(set([ent.text for ent in doc.ents if ent.label_ == \"DATE\"]))\n",
    "        section_features[heading] = {\n",
    "            \"noun_chunks\": noun_chunks,\n",
    "            \"compounds\": compounds,\n",
    "            \"verbal_nouns\": verbal_nouns,\n",
    "            \"verbs\": verbs,\n",
    "            \"dates\": dates\n",
    "        }\n",
    "    return section_features\n",
    "\n",
    "# ================================\n",
    "# DRIVER\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"9 word resume.pdf\"\n",
    "    extracted_output = extract_text(file_path)\n",
    "\n",
    "    if isinstance(extracted_output, tuple):\n",
    "        final_blocks, page_chunks = extracted_output\n",
    "    else:\n",
    "        final_blocks = extracted_output\n",
    "        page_chunks = []\n",
    "\n",
    "    print(f\"\\nParsed Output from: {os.path.basename(file_path)}\")\n",
    "    print(\"\\n================ RAW PARSED TEXT BLOCKS ================\\n\")\n",
    "    for block in final_blocks:\n",
    "        print(block)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n================ SEMANTIC SECTIONING OUTPUT ================\\n\")\n",
    "    sections = semantic_sectioning(final_blocks)\n",
    "    for heading, content in sections.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\\n{content}\\n\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n================ SPA-CY FEATURES PER SECTION ================\\n\")\n",
    "    section_features = extract_section_spacy_features(sections)\n",
    "    for heading, feats in section_features.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\")\n",
    "        print(\"Noun Chunks: \", \", \".join(feats[\"noun_chunks\"]))\n",
    "        print(\"Compound Nouns: \", \", \".join(feats[\"compounds\"]))\n",
    "        print(\"Verbal Nouns: \", \", \".join(feats[\"verbal_nouns\"]))\n",
    "        print(\"Verbs: \", \", \".join(feats[\"verbs\"]))\n",
    "        print(\"Dates: \", \", \".join(feats[\"dates\"]))\n",
    "        print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ffc93-96ed-413e-8ab5-c3c4e6c00cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new code for spacy test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a651d4e-10fc-45e5-9e8f-20e8623be1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed Output from: 3372246-student-nurse-resume-with-clinical-experience.pdf\n",
      "\n",
      "================ RAW PARSED TEXT BLOCKS ================\n",
      "\n",
      "# Daniel Kim\n",
      "----------------------------------------\n",
      "I am a student nurse with extensive clinical rotation experience across various healthcare\n",
      "settings. I am passionate about patient care and committed to applying clinical best\n",
      "practices in a fast-paced environment.\n",
      "----------------------------------------\n",
      "## **Professional Experience**\n",
      "----------------------------------------\n",
      "CLINICAL ROTATIONS [|] MULTIPLE HOSPITALS, HOUSTON, TX\n",
      "JANUARY 2023 – PRESENT\n",
      "----------------------------------------\n",
      "## • Completed over 300 clinical hours in medical-surgical, ICU, and pediatric units. • Assisted RNs with patient assessments and medication administration.\n",
      "----------------------------------------\n",
      "VOLUNTEER NURSING ASSISTANT [|] HOUSTON COMMUNITY CLINIC, HOUSTON, TX\n",
      "JUNE 2022 – DECEMBER 2022\n",
      "----------------------------------------\n",
      "## • Provided bedside care and support to 10+ patients per shift. • Documented patient progress and collaborated with the healthcare team.\n",
      "----------------------------------------\n",
      "Assisted RNs with patient assessments and medication administration.\n",
      "•\n",
      "----------------------------------------\n",
      "Contact\n",
      "----------------------------------------\n",
      "(555) 444-5555\n",
      "----------------------------------------\n",
      "daniel.kim@example.com\n",
      "----------------------------------------\n",
      "LinkedIn\n",
      "----------------------------------------\n",
      "Houston, TX 77001\n",
      "----------------------------------------\n",
      "Key Skills\n",
      "----------------------------------------\n",
      "Clinical Assessment\n",
      "•\n",
      "----------------------------------------\n",
      "Patient Care\n",
      "•\n",
      "----------------------------------------\n",
      "Medication Administration\n",
      "•\n",
      "----------------------------------------\n",
      "Documentation\n",
      "•\n",
      "----------------------------------------\n",
      "Education\n",
      "----------------------------------------\n",
      "Bachelor of Science in Nursing\n",
      "(BSN)\n",
      "----------------------------------------\n",
      "\n",
      "================ SEMANTIC SECTIONING OUTPUT ================\n",
      "\n",
      "\n",
      "### ## **PROFESSIONAL EXPERIENCE** ###\n",
      "CLINICAL ROTATIONS [|] MULTIPLE HOSPITALS, HOUSTON, TX\n",
      "JANUARY 2023 – PRESENT\n",
      "## • Completed over 300 clinical hours in medical-surgical, ICU, and pediatric units. • Assisted RNs with patient assessments and medication administration.\n",
      "VOLUNTEER NURSING ASSISTANT [|] HOUSTON COMMUNITY CLINIC, HOUSTON, TX\n",
      "JUNE 2022 – DECEMBER 2022\n",
      "## • Provided bedside care and support to 10+ patients per shift. • Documented patient progress and collaborated with the healthcare team.\n",
      "Assisted RNs with patient assessments and medication administration.\n",
      "•\n",
      "\n",
      "============================================================\n",
      "\n",
      "### CONTACT ###\n",
      "(555) 444-5555\n",
      "daniel.kim@example.com\n",
      "LinkedIn\n",
      "Houston, TX 77001\n",
      "\n",
      "============================================================\n",
      "\n",
      "### KEY SKILLS ###\n",
      "Clinical Assessment\n",
      "•\n",
      "Patient Care\n",
      "•\n",
      "Medication Administration\n",
      "•\n",
      "Documentation\n",
      "•\n",
      "\n",
      "============================================================\n",
      "\n",
      "### EDUCATION ###\n",
      "Bachelor of Science in Nursing\n",
      "(BSN)\n",
      "\n",
      "============================================================\n",
      "\n",
      "================ SPA-CY FEATURES PER SECTION ================\n",
      "\n",
      "\n",
      "### ## **PROFESSIONAL EXPERIENCE** ###\n",
      "Noun Chunks:  clinical rotations, [|] multiple hospitals, houston, tx january, ##, 300 clinical hours, icu, pediatric units, rns, patient assessments, medication administration, volunteer nursing assistant, [|] houston community clinic, december, bedside care, support, patients, shift, documented patient progress, the healthcare team\n",
      "Compound Nouns:  clinical rotations, tx january, medication administration, volunteer nursing assistant, [|] houston community clinic, bedside care, the healthcare team\n",
      "Verbal Nouns:  \n",
      "Verbs:  assist, collaborate, complete, document, provide\n",
      "Dates:  DECEMBER\n",
      "============================================================\n",
      "\n",
      "### CONTACT ###\n",
      "Noun Chunks:  (555) 444-5555 daniel.kim@example.com linkedin houston\n",
      "Compound Nouns:  (555) 444-5555 daniel.kim@example.com linkedin houston\n",
      "Verbal Nouns:  \n",
      "Verbs:  \n",
      "Dates:  \n",
      "============================================================\n",
      "\n",
      "### KEY SKILLS ###\n",
      "Noun Chunks:  clinical assessment • patient care • medication administration, documentation\n",
      "Compound Nouns:  clinical assessment • patient care • medication administration\n",
      "Verbal Nouns:  \n",
      "Verbs:  \n",
      "Dates:  \n",
      "============================================================\n",
      "\n",
      "### EDUCATION ###\n",
      "Noun Chunks:  bachelor, science, nursing, bsn\n",
      "Compound Nouns:  \n",
      "Verbal Nouns:  \n",
      "Verbs:  \n",
      "Dates:  \n",
      "============================================================\n",
      "\n",
      "================ ORDERED TAGGED SEMANTIC EXTRACT ================\n",
      "\n",
      "\n",
      "### ## **PROFESSIONAL EXPERIENCE** ###\n",
      "\n",
      "[CLINICAL ROTATIONS|noun_chunks,compounds] [|] MULTIPLE HOSPITALS, [HOUSTON|noun_chunks], TX\n",
      "JANUARY 2023 – PRESENT\n",
      "## • Completed over [300 clinical hours|noun_chunks] in medical-surgical, [ICU|noun_chunks], and [pediatric units|noun_chunks]. • Assisted [RNs|noun_chunks] with [patient assessments|noun_chunks] and [medication administration|noun_chunks,compounds].\n",
      "[VOLUNTEER NURSING ASSISTANT|noun_chunks,compounds] [|] [HOUSTON|noun_chunks] COMMUNITY CLINIC, [HOUSTON|noun_chunks], TX\n",
      "JUNE 2022 – [DECEMBER|noun_chunks,dates] 2022\n",
      "## • Provided [bedside care|noun_chunks,compounds] and [support|noun_chunks] to 10+ [patients|noun_chunks] per [shift|noun_chunks]. • [Documented patient progress|noun_chunks] and collaborated with [the healthcare team|noun_chunks,compounds].\n",
      "Assisted [RNs|noun_chunks] with [patient assessments|noun_chunks] and [medication administration|noun_chunks,compounds].\n",
      "•\n",
      "============================================================\n",
      "\n",
      "### CONTACT ###\n",
      "\n",
      "(555) 444-5555\n",
      "daniel.kim@example.com\n",
      "LinkedIn\n",
      "Houston, TX 77001\n",
      "============================================================\n",
      "\n",
      "### KEY SKILLS ###\n",
      "\n",
      "Clinical Assessment\n",
      "•\n",
      "Patient Care\n",
      "•\n",
      "Medication Administration\n",
      "•\n",
      "[Documentation|noun_chunks]\n",
      "•\n",
      "============================================================\n",
      "\n",
      "### EDUCATION ###\n",
      "\n",
      "[Bachelor|noun_chunks] of [Science|noun_chunks] in [Nursing|noun_chunks]\n",
      "([BSN|noun_chunks])\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pymupdf4llm\n",
    "from docx2python import docx2python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "\n",
    "# ================================\n",
    "# UNIVERSAL TEXT EXTRACTION\n",
    "# ================================\n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_pdf_blocks(file_path)\n",
    "    elif ext in [\".docx\", \".doc\"]:\n",
    "        return extract_docx_exact_layout(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Use PDF or DOCX.\")\n",
    "\n",
    "def extract_docx_exact_layout(filepath):\n",
    "    doc_result = docx2python(filepath)\n",
    "    main_content = doc_result.body\n",
    "    all_text = []\n",
    "    for section in main_content:\n",
    "        for row in section:\n",
    "            for cell in row:\n",
    "                for para in cell:\n",
    "                    para_str = para.strip()\n",
    "                    if para_str:\n",
    "                        all_text.append(para_str)\n",
    "    return all_text\n",
    "\n",
    "def extract_column_aware_blocks(pdf_path, column_gap=50):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_blocks = []\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"blocks\", sort=True)\n",
    "        blocks = sorted(blocks, key=lambda b: (b[0], b[1]))\n",
    "        left_col, right_col = [], []\n",
    "        if blocks:\n",
    "            page_width = page.rect.width\n",
    "            center_line = page_width / 2\n",
    "            for b in blocks:\n",
    "                x0, y0, x1, y1, text, *_ = b\n",
    "                if x1 < center_line - column_gap:\n",
    "                    left_col.append((y0, text.strip()))\n",
    "                else:\n",
    "                    right_col.append((y0, text.strip()))\n",
    "            left_col_sorted = [t for _, t in sorted(left_col)]\n",
    "            right_col_sorted = [t for _, t in sorted(right_col)]\n",
    "            combined = right_col_sorted + left_col_sorted\n",
    "            all_blocks.extend([t for t in combined if t])\n",
    "    doc.close()\n",
    "    return all_blocks\n",
    "\n",
    "def extract_pdf_blocks(pdf_path):\n",
    "    blocks_code1 = extract_column_aware_blocks(pdf_path)\n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "    blocks_code2 = [\n",
    "        block.strip()\n",
    "        for block in md_text.split(\"\\n\\n\")\n",
    "        if block.strip() and len(block.strip()) > 5\n",
    "    ]\n",
    "    md_chunks = pymupdf4llm.to_markdown(pdf_path, page_chunks=True)\n",
    "    page_chunk_blocks = [\n",
    "        chunk[\"text\"].strip()\n",
    "        for chunk in md_chunks\n",
    "        if isinstance(chunk, dict) and \"text\" in chunk and chunk[\"text\"].strip()\n",
    "    ]\n",
    "\n",
    "    def jaccard_similarity(s1, s2, threshold=0.7):\n",
    "        set1, set2 = set(s1.lower().strip()), set(s2.lower().strip())\n",
    "        intersection = set1 & set2\n",
    "        union = set1 | set2\n",
    "        if not union:\n",
    "            return False\n",
    "        return len(intersection) / len(union) > threshold\n",
    "\n",
    "    missing_from_code2 = []\n",
    "    for block1 in blocks_code1:\n",
    "        if not any(jaccard_similarity(block1, block2) for block2 in blocks_code2):\n",
    "            missing_from_code2.append(block1)\n",
    "    final_blocks = blocks_code2 + missing_from_code2\n",
    "    return final_blocks, page_chunk_blocks\n",
    "\n",
    "# ================================\n",
    "# SEMANTIC HEADING DETECTION + SECTION GROUPING\n",
    "# ================================\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "canonical_headings = [\n",
    "    \"experience\", \"work experience\", \"education\", \"academic history\",\n",
    "    \"projects\", \"technical projects\", \"skills\", \"key skills\",\n",
    "    \"certifications\", \"achievements\", \"publications\", \"personal details\", \"contact\",\n",
    "    \"technical skills\", \"professional experience\",\"Core competencies\"\n",
    "]\n",
    "canonical_embeddings = model.encode(canonical_headings, convert_to_tensor=True)\n",
    "\n",
    "def semantic_heading_detection(lines, threshold=0.6):\n",
    "    headings = []\n",
    "    for line in lines:\n",
    "        line_clean = line.strip().lower()\n",
    "        if not line_clean or len(line_clean) < 3:\n",
    "            continue\n",
    "        emb = model.encode(line_clean, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(emb, canonical_embeddings)[0]\n",
    "        max_score = float(cosine_scores.max())\n",
    "        if max_score >= threshold:\n",
    "            headings.append(line.strip())\n",
    "    return headings\n",
    "\n",
    "def semantic_sectioning(lines, threshold=0.55):\n",
    "    detected_headings = semantic_heading_detection(lines, threshold)\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    buffer = []\n",
    "    for line in lines:\n",
    "        if line.strip() in detected_headings:\n",
    "            if current_section and buffer:\n",
    "                sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "            current_section = line.strip()\n",
    "            buffer = []\n",
    "        else:\n",
    "            buffer.append(line.strip())\n",
    "    if current_section and buffer:\n",
    "        sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "    return sections\n",
    "\n",
    "# ================================\n",
    "# SPA-CY FEATURE EXTRACTION\n",
    "# ================================\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def filter_and_clean_noun_chunks(doc):\n",
    "    seen = set()\n",
    "    clean_chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        text = chunk.text.strip().lower()\n",
    "        if not text or len(text) < 2:\n",
    "            continue\n",
    "        if all(token.is_stop for token in chunk):\n",
    "            continue\n",
    "        if text in seen:\n",
    "            continue\n",
    "        seen.add(text)\n",
    "        clean_chunks.append(text)\n",
    "    return clean_chunks\n",
    "\n",
    "def extract_section_spacy_features(sections):\n",
    "    section_features = {}\n",
    "    for heading, text in sections.items():\n",
    "        flat = \" \".join(line.strip() for line in text.splitlines() if line.strip())\n",
    "        doc = nlp(flat)\n",
    "        noun_chunks = filter_and_clean_noun_chunks(doc)\n",
    "        verbs = sorted(set([t.lemma_ for t in doc if t.pos_ == \"VERB\" and not t.is_stop and len(t.lemma_) > 1]))\n",
    "        compounds = []\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if any(t.dep_ == \"compound\" for t in chunk):\n",
    "                compound_text = chunk.text.strip().lower()\n",
    "                if compound_text not in compounds:\n",
    "                    compounds.append(compound_text)\n",
    "        verbal_nouns = sorted(set([t.text for t in doc if t.tag_ == \"VBG\" and t.pos_ == \"NOUN\"]))\n",
    "        dates = sorted(set([ent.text for ent in doc.ents if ent.label_ == \"DATE\"]))\n",
    "        section_features[heading] = {\n",
    "            \"noun_chunks\": noun_chunks,\n",
    "            \"compounds\": compounds,\n",
    "            \"verbal_nouns\": verbal_nouns,\n",
    "            \"verbs\": verbs,\n",
    "            \"dates\": dates\n",
    "        }\n",
    "    return section_features\n",
    "\n",
    "# ================================\n",
    "# MAIN - ORDERED TAGGED EXTRACT\n",
    "# ================================\n",
    "def tag_section_features_in_order(sections, section_features):\n",
    "    for heading, text in sections.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\\n\")\n",
    "        # For each sentence/line, scan for presence of feature and tag for each word/phrase\n",
    "        lines = text.splitlines()\n",
    "        features = section_features[heading]\n",
    "        \n",
    "        # Get all unique possible feature strings (lowercase for matching)\n",
    "        feat_types = ['noun_chunks', 'compounds', 'verbal_nouns', 'verbs', 'dates']\n",
    "        feat_map = {}\n",
    "        for ft in feat_types:\n",
    "            for val in features[ft]:\n",
    "                val_low = val.lower().strip()\n",
    "                if val_low not in feat_map:\n",
    "                    feat_map[val_low] = []\n",
    "                feat_map[val_low].append(ft)\n",
    "        \n",
    "        # For each line, scan word-by-word, tag feature matches (longest first to avoid substrings issue)\n",
    "        import re\n",
    "        # Collect all feature phrases, sorted by length descending\n",
    "        phrases_sorted = sorted(feat_map.keys(), key=lambda x: -len(x))\n",
    "        for line in lines:\n",
    "            line_str = line.strip()\n",
    "            output = line_str\n",
    "            for phrase in phrases_sorted:\n",
    "                if phrase and phrase in line_str.lower():\n",
    "                    tag = ','.join(feat_map[phrase])\n",
    "                    # Use word boundary for more precise matching\n",
    "                    pat = r'(?i)\\b({})\\b'.format(re.escape(phrase))\n",
    "                    output = re.sub(pat, r'[\\1|{}]'.format(tag), output)\n",
    "            print(output)\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"3372246-student-nurse-resume-with-clinical-experience.pdf\"\n",
    "    extracted_output = extract_text(file_path)\n",
    "\n",
    "    if isinstance(extracted_output, tuple):\n",
    "        final_blocks, page_chunks = extracted_output\n",
    "    else:\n",
    "        final_blocks = extracted_output\n",
    "        page_chunks = []\n",
    "\n",
    "    print(f\"\\nParsed Output from: {os.path.basename(file_path)}\")\n",
    "    print(\"\\n================ RAW PARSED TEXT BLOCKS ================\\n\")\n",
    "    for block in final_blocks:\n",
    "        print(block)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n================ SEMANTIC SECTIONING OUTPUT ================\\n\")\n",
    "    sections = semantic_sectioning(final_blocks)\n",
    "    for heading, content in sections.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\\n{content}\\n\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n================ SPA-CY FEATURES PER SECTION ================\\n\")\n",
    "    section_features = extract_section_spacy_features(sections)\n",
    "    for heading, feats in section_features.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\")\n",
    "        print(\"Noun Chunks: \", \", \".join(feats[\"noun_chunks\"]))\n",
    "        print(\"Compound Nouns: \", \", \".join(feats[\"compounds\"]))\n",
    "        print(\"Verbal Nouns: \", \", \".join(feats[\"verbal_nouns\"]))\n",
    "        print(\"Verbs: \", \", \".join(feats[\"verbs\"]))\n",
    "        print(\"Dates: \", \", \".join(feats[\"dates\"]))\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n================ ORDERED TAGGED SEMANTIC EXTRACT ================\\n\")\n",
    "    tag_section_features_in_order(sections, section_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d5b01-c388-43b9-b4a6-810743d2c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48d3ed1b-9ea8-448a-bb52-7aeeb574ca54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ RAW PARSED TEXT BLOCKS ================\n",
      "\n",
      "# Daniel Kim\n",
      "----------------------------------------\n",
      "I am a student nurse with extensive clinical rotation experience across various healthcare\n",
      "settings. I am passionate about patient care and committed to applying clinical best\n",
      "practices in a fast-paced environment.\n",
      "----------------------------------------\n",
      "## **Professional Experience**\n",
      "----------------------------------------\n",
      "CLINICAL ROTATIONS [|] MULTIPLE HOSPITALS, HOUSTON, TX\n",
      "JANUARY 2023 – PRESENT\n",
      "----------------------------------------\n",
      "## • Completed over 300 clinical hours in medical-surgical, ICU, and pediatric units. • Assisted RNs with patient assessments and medication administration.\n",
      "----------------------------------------\n",
      "VOLUNTEER NURSING ASSISTANT [|] HOUSTON COMMUNITY CLINIC, HOUSTON, TX\n",
      "JUNE 2022 – DECEMBER 2022\n",
      "----------------------------------------\n",
      "## • Provided bedside care and support to 10+ patients per shift. • Documented patient progress and collaborated with the healthcare team.\n",
      "----------------------------------------\n",
      "Assisted RNs with patient assessments and medication administration.\n",
      "•\n",
      "----------------------------------------\n",
      "Contact\n",
      "----------------------------------------\n",
      "(555) 444-5555\n",
      "----------------------------------------\n",
      "daniel.kim@example.com\n",
      "----------------------------------------\n",
      "LinkedIn\n",
      "----------------------------------------\n",
      "Houston, TX 77001\n",
      "----------------------------------------\n",
      "Key Skills\n",
      "----------------------------------------\n",
      "Clinical Assessment\n",
      "•\n",
      "----------------------------------------\n",
      "Patient Care\n",
      "•\n",
      "----------------------------------------\n",
      "Medication Administration\n",
      "•\n",
      "----------------------------------------\n",
      "Documentation\n",
      "•\n",
      "----------------------------------------\n",
      "Education\n",
      "----------------------------------------\n",
      "Bachelor of Science in Nursing\n",
      "(BSN)\n",
      "----------------------------------------\n",
      "\n",
      "================ SEMANTIC SECTIONING OUTPUT ================\n",
      "\n",
      "\n",
      "### ## **PROFESSIONAL EXPERIENCE** ###\n",
      "CLINICAL ROTATIONS [|] MULTIPLE HOSPITALS, HOUSTON, TX\n",
      "JANUARY 2023 – PRESENT\n",
      "## • Completed over 300 clinical hours in medical-surgical, ICU, and pediatric units. • Assisted RNs with patient assessments and medication administration.\n",
      "VOLUNTEER NURSING ASSISTANT [|] HOUSTON COMMUNITY CLINIC, HOUSTON, TX\n",
      "JUNE 2022 – DECEMBER 2022\n",
      "## • Provided bedside care and support to 10+ patients per shift. • Documented patient progress and collaborated with the healthcare team.\n",
      "Assisted RNs with patient assessments and medication administration.\n",
      "•\n",
      "\n",
      "============================================================\n",
      "\n",
      "### CONTACT ###\n",
      "(555) 444-5555\n",
      "daniel.kim@example.com\n",
      "LinkedIn\n",
      "Houston, TX 77001\n",
      "\n",
      "============================================================\n",
      "\n",
      "### KEY SKILLS ###\n",
      "Clinical Assessment\n",
      "•\n",
      "Patient Care\n",
      "•\n",
      "Medication Administration\n",
      "•\n",
      "Documentation\n",
      "•\n",
      "\n",
      "============================================================\n",
      "\n",
      "### EDUCATION ###\n",
      "Bachelor of Science in Nursing\n",
      "(BSN)\n",
      "\n",
      "============================================================\n",
      "\n",
      "================ SPA-CY FEATURES PER SECTION ================\n",
      "\n",
      "\n",
      "### ## **PROFESSIONAL EXPERIENCE** ###\n",
      "Noun Chunks:  clinical rotations, [|] multiple hospitals, houston, tx january, ##, 300 clinical hours, icu, pediatric units, rns, patient assessments, medication administration, volunteer nursing assistant, [|] houston community clinic, december, bedside care, support, patients, shift, documented patient progress, the healthcare team\n",
      "Compound Nouns:  clinical rotations, tx january, medication administration, volunteer nursing assistant, [|] houston community clinic, bedside care, the healthcare team\n",
      "Verbal Nouns:  \n",
      "Verbs:  assist, collaborate, complete, document, provide\n",
      "Dates:  DECEMBER\n",
      "============================================================\n",
      "\n",
      "### CONTACT ###\n",
      "Noun Chunks:  (555) 444-5555 daniel.kim@example.com linkedin houston\n",
      "Compound Nouns:  (555) 444-5555 daniel.kim@example.com linkedin houston\n",
      "Verbal Nouns:  \n",
      "Verbs:  \n",
      "Dates:  \n",
      "============================================================\n",
      "\n",
      "### KEY SKILLS ###\n",
      "Noun Chunks:  clinical assessment • patient care • medication administration, documentation\n",
      "Compound Nouns:  clinical assessment • patient care • medication administration\n",
      "Verbal Nouns:  \n",
      "Verbs:  \n",
      "Dates:  \n",
      "============================================================\n",
      "\n",
      "### EDUCATION ###\n",
      "Noun Chunks:  bachelor, science, nursing, bsn\n",
      "Compound Nouns:  \n",
      "Verbal Nouns:  \n",
      "Verbs:  \n",
      "Dates:  \n",
      "============================================================\n",
      "\n",
      "================ FEATURES ONLY FROM ORDERED TAGGED SEMANTIC EXTRACT ================\n",
      "\n",
      "\n",
      "### ## **PROFESSIONAL EXPERIENCE** ###\n",
      "\n",
      "[CLINICAL ROTATIONS|noun_chunks,compounds] [HOUSTON|noun_chunks]\n",
      "\n",
      "[medication administration|noun_chunks,compounds] [patient assessments|noun_chunks] [300 clinical hours|noun_chunks] [pediatric units|noun_chunks] [ICU|noun_chunks] [RNs|noun_chunks]\n",
      "[VOLUNTEER NURSING ASSISTANT|noun_chunks,compounds] [HOUSTON|noun_chunks] [HOUSTON|noun_chunks]\n",
      "[DECEMBER|noun_chunks,dates]\n",
      "[Documented patient progress|noun_chunks] [the healthcare team|noun_chunks,compounds] [bedside care|noun_chunks,compounds] [patients|noun_chunks] [support|noun_chunks] [shift|noun_chunks]\n",
      "[medication administration|noun_chunks,compounds] [patient assessments|noun_chunks] [RNs|noun_chunks]\n",
      "\n",
      "============================================================\n",
      "\n",
      "### CONTACT ###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "### KEY SKILLS ###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Documentation|noun_chunks]\n",
      "\n",
      "============================================================\n",
      "\n",
      "### EDUCATION ###\n",
      "\n",
      "[Bachelor|noun_chunks] [Science|noun_chunks] [Nursing|noun_chunks]\n",
      "[BSN|noun_chunks]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pymupdf4llm\n",
    "from docx2python import docx2python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "\n",
    "# ================================\n",
    "# UNIVERSAL TEXT EXTRACTION\n",
    "# ================================\n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_pdf_blocks(file_path)\n",
    "    elif ext in [\".docx\", \".doc\"]:\n",
    "        return extract_docx_exact_layout(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Use PDF or DOCX.\")\n",
    "\n",
    "def extract_docx_exact_layout(filepath):\n",
    "    doc_result = docx2python(filepath)\n",
    "    main_content = doc_result.body\n",
    "    all_text = []\n",
    "    for section in main_content:\n",
    "        for row in section:\n",
    "            for cell in row:\n",
    "                for para in cell:\n",
    "                    para_str = para.strip()\n",
    "                    if para_str:\n",
    "                        all_text.append(para_str)\n",
    "    return all_text\n",
    "\n",
    "def extract_column_aware_blocks(pdf_path, column_gap=50):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_blocks = []\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"blocks\", sort=True)\n",
    "        blocks = sorted(blocks, key=lambda b: (b[0], b[1]))\n",
    "        left_col, right_col = [], []\n",
    "        if blocks:\n",
    "            page_width = page.rect.width\n",
    "            center_line = page_width / 2\n",
    "            for b in blocks:\n",
    "                x0, y0, x1, y1, text, *_ = b\n",
    "                if x1 < center_line - column_gap:\n",
    "                    left_col.append((y0, text.strip()))\n",
    "                else:\n",
    "                    right_col.append((y0, text.strip()))\n",
    "            left_col_sorted = [t for _, t in sorted(left_col)]\n",
    "            right_col_sorted = [t for _, t in sorted(right_col)]\n",
    "            combined = right_col_sorted + left_col_sorted\n",
    "            all_blocks.extend([t for t in combined if t])\n",
    "    doc.close()\n",
    "    return all_blocks\n",
    "\n",
    "def extract_pdf_blocks(pdf_path):\n",
    "    blocks_code1 = extract_column_aware_blocks(pdf_path)\n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "    blocks_code2 = [\n",
    "        block.strip()\n",
    "        for block in md_text.split(\"\\n\\n\")\n",
    "        if block.strip() and len(block.strip()) > 5\n",
    "    ]\n",
    "    md_chunks = pymupdf4llm.to_markdown(pdf_path, page_chunks=True)\n",
    "    page_chunk_blocks = [\n",
    "        chunk[\"text\"].strip()\n",
    "        for chunk in md_chunks\n",
    "        if isinstance(chunk, dict) and \"text\" in chunk and chunk[\"text\"].strip()\n",
    "    ]\n",
    "\n",
    "    def jaccard_similarity(s1, s2, threshold=0.7):\n",
    "        set1, set2 = set(s1.lower().strip()), set(s2.lower().strip())\n",
    "        intersection = set1 & set2\n",
    "        union = set1 | set2\n",
    "        if not union:\n",
    "            return False\n",
    "        return len(intersection) / len(union) > threshold\n",
    "\n",
    "    missing_from_code2 = []\n",
    "    for block1 in blocks_code1:\n",
    "        if not any(jaccard_similarity(block1, block2) for block2 in blocks_code2):\n",
    "            missing_from_code2.append(block1)\n",
    "    final_blocks = blocks_code2 + missing_from_code2\n",
    "    return final_blocks, page_chunk_blocks\n",
    "\n",
    "# ================================\n",
    "# SEMANTIC HEADING DETECTION + SECTION GROUPING\n",
    "# ================================\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "canonical_headings = [\n",
    "    \"experience\", \"work experience\", \"education\", \"academic history\",\n",
    "    \"projects\", \"technical projects\", \"skills\", \"key skills\",\n",
    "    \"certifications\", \"achievements\", \"publications\", \"personal details\", \"contact\",\n",
    "    \"technical skills\", \"professional experience\",\"Core competencies\"\n",
    "]\n",
    "canonical_embeddings = model.encode(canonical_headings, convert_to_tensor=True)\n",
    "\n",
    "def semantic_heading_detection(lines, threshold=0.6):\n",
    "    headings = []\n",
    "    for line in lines:\n",
    "        line_clean = line.strip().lower()\n",
    "        if not line_clean or len(line_clean) < 3:\n",
    "            continue\n",
    "        emb = model.encode(line_clean, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(emb, canonical_embeddings)[0]\n",
    "        max_score = float(cosine_scores.max())\n",
    "        if max_score >= threshold:\n",
    "            headings.append(line.strip())\n",
    "    return headings\n",
    "\n",
    "def semantic_sectioning(lines, threshold=0.55):\n",
    "    detected_headings = semantic_heading_detection(lines, threshold)\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    buffer = []\n",
    "    for line in lines:\n",
    "        if line.strip() in detected_headings:\n",
    "            if current_section and buffer:\n",
    "                sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "            current_section = line.strip()\n",
    "            buffer = []\n",
    "        else:\n",
    "            buffer.append(line.strip())\n",
    "    if current_section and buffer:\n",
    "        sections[current_section] = \"\\n\".join(buffer).strip()\n",
    "    return sections\n",
    "\n",
    "# ================================\n",
    "# SPA-CY FEATURE EXTRACTION\n",
    "# ================================\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def filter_and_clean_noun_chunks(doc):\n",
    "    seen = set()\n",
    "    clean_chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        text = chunk.text.strip().lower()\n",
    "        if not text or len(text) < 2:\n",
    "            continue\n",
    "        if all(token.is_stop for token in chunk):\n",
    "            continue\n",
    "        if text in seen:\n",
    "            continue\n",
    "        seen.add(text)\n",
    "        clean_chunks.append(text)\n",
    "    return clean_chunks\n",
    "\n",
    "def extract_section_spacy_features(sections):\n",
    "    section_features = {}\n",
    "    for heading, text in sections.items():\n",
    "        flat = \" \".join(line.strip() for line in text.splitlines() if line.strip())\n",
    "        doc = nlp(flat)\n",
    "        noun_chunks = filter_and_clean_noun_chunks(doc)\n",
    "        verbs = sorted(set([t.lemma_ for t in doc if t.pos_ == \"VERB\" and not t.is_stop and len(t.lemma_) > 1]))\n",
    "        compounds = []\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if any(t.dep_ == \"compound\" for t in chunk):\n",
    "                compound_text = chunk.text.strip().lower()\n",
    "                if compound_text not in compounds:\n",
    "                    compounds.append(compound_text)\n",
    "        verbal_nouns = sorted(set([t.text for t in doc if t.tag_ == \"VBG\" and t.pos_ == \"NOUN\"]))\n",
    "        dates = sorted(set([ent.text for ent in doc.ents if ent.label_ == \"DATE\"]))\n",
    "        section_features[heading] = {\n",
    "            \"noun_chunks\": noun_chunks,\n",
    "            \"compounds\": compounds,\n",
    "            \"verbal_nouns\": verbal_nouns,\n",
    "            \"verbs\": verbs,\n",
    "            \"dates\": dates\n",
    "        }\n",
    "    return section_features\n",
    "\n",
    "# ================================\n",
    "# MAIN - ONLY FEATURED TERMS OUTPUT\n",
    "# ================================\n",
    "def extract_tagged_features_only(sections, section_features):\n",
    "    import re\n",
    "    for heading, text in sections.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\\n\")\n",
    "        lines = text.splitlines()\n",
    "        features = section_features[heading]\n",
    "\n",
    "        # Build map of feature strings to type tags\n",
    "        feat_types = ['noun_chunks', 'compounds', 'verbal_nouns', 'verbs', 'dates']\n",
    "        feat_map = {}\n",
    "        for ft in feat_types:\n",
    "            for val in features[ft]:\n",
    "                val_low = val.lower().strip()\n",
    "                if val_low not in feat_map:\n",
    "                    feat_map[val_low] = []\n",
    "                feat_map[val_low].append(ft)\n",
    "\n",
    "        # Sort phrases descending by length for best matching\n",
    "        phrases_sorted = sorted(feat_map.keys(), key=lambda x: -len(x))\n",
    "        for line in lines:\n",
    "            line_str = line.strip()\n",
    "            output = []\n",
    "            # For each phrase, use regex to find all matches in original order\n",
    "            used_spans = []\n",
    "            for phrase in phrases_sorted:\n",
    "                pat = r'(?i)\\b({})\\b'.format(re.escape(phrase))\n",
    "                for m in re.finditer(pat, line_str):\n",
    "                    span = m.span()\n",
    "                    # Avoid overlapping matches\n",
    "                    if any(span[0] < u[1] and span[1] > u[0] for u in used_spans):\n",
    "                        continue\n",
    "                    match_text = m.group(1)\n",
    "                    tag = ','.join(feat_map[phrase])\n",
    "                    output.append(f\"[{match_text}|{tag}]\")\n",
    "                    used_spans.append(span)\n",
    "            print(' '.join(output))\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"3372246-student-nurse-resume-with-clinical-experience.pdf\"\n",
    "    extracted_output = extract_text(file_path)\n",
    "\n",
    "    if isinstance(extracted_output, tuple):\n",
    "        final_blocks, page_chunks = extracted_output\n",
    "    else:\n",
    "        final_blocks = extracted_output\n",
    "        page_chunks = []\n",
    "\n",
    "    print(\"\\n================ RAW PARSED TEXT BLOCKS ================\\n\")\n",
    "    for block in final_blocks:\n",
    "        print(block)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n================ SEMANTIC SECTIONING OUTPUT ================\\n\")\n",
    "    sections = semantic_sectioning(final_blocks)\n",
    "    for heading, content in sections.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\\n{content}\\n\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n================ SPA-CY FEATURES PER SECTION ================\\n\")\n",
    "    section_features = extract_section_spacy_features(sections)\n",
    "    for heading, feats in section_features.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\")\n",
    "        print(\"Noun Chunks: \", \", \".join(feats[\"noun_chunks\"]))\n",
    "        print(\"Compound Nouns: \", \", \".join(feats[\"compounds\"]))\n",
    "        print(\"Verbal Nouns: \", \", \".join(feats[\"verbal_nouns\"]))\n",
    "        print(\"Verbs: \", \", \".join(feats[\"verbs\"]))\n",
    "        print(\"Dates: \", \", \".join(feats[\"dates\"]))\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n================ FEATURES ONLY FROM ORDERED TAGGED SEMANTIC EXTRACT ================\\n\")\n",
    "    extract_tagged_features_only(sections, section_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef52d6-576f-4c2c-bc2b-4f28bb75a443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_kernal",
   "language": "python",
   "name": "spacy_kernal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
