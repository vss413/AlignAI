{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef5be4-10cd-4b76-b199-c22bbd8f2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "the developing code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "183291bd-bfdd-499c-a9cc-dae606a98e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ RESUME SECTIONS ================\n",
      "\n",
      "\n",
      "### ## **PROFESSIONAL EXPERIENCE** (1) ###\n",
      "Server\n",
      "Bistro 56, Chicago, IL | May 2020 - Present\n",
      "Provide service to 80+ customers daily, consistently achieving a guest\n",
      "satisfaction rating of 95%\n",
      "Actively upsell daily specials and premium beverages, contributing to a 10%\n",
      "increase in average check amounts\n",
      "Collaborate with kitchen and front-of-house teams to deliver fast and efficient\n",
      "\n",
      "============================================================\n",
      "\n",
      "### SERVICE (1) ###\n",
      "Server\n",
      "Downtown Diner, Chicago, IL | June 2017 - April 2020\n",
      "Ensured guest orders were accurate and delivered in a timely manner,\n",
      "achieving consistent positive feedback\n",
      "Assisted in managing guest complaints and resolving issues promptly to\n",
      "maintain high satisfaction levels\n",
      "Maintained cleanliness and organization of dining areas, supporting overall\n",
      "restaurant efficiency\n",
      "\n",
      "============================================================\n",
      "\n",
      "### HIGH SCHOOL DIPLOMA (1) ###\n",
      "Chicago High School, Chicago, IL | May 2017\n",
      "\n",
      "============================================================\n",
      "\n",
      "### CONTACT (1) ###\n",
      "(555) 234-5678\n",
      "rachelmatthews@email.com\n",
      "LinkedIn | Portfolio\n",
      "Chicago, IL 60601\n",
      "\n",
      "============================================================\n",
      "\n",
      "### CUSTOMER SERVICE (1) ###\n",
      "Upselling techniques\n",
      "Guest relations\n",
      "Time management\n",
      "Multi-tasking\n",
      "\n",
      "============================================================\n",
      "\n",
      "### CERTIFICATIONS (1) ###\n",
      "ServSafe Food Handler\n",
      "Certification, May 2018\n",
      "\n",
      "============================================================\n",
      "\n",
      "================ JOB DESCRIPTION SECTIONS ================\n",
      "\n",
      "\n",
      "### JOB DESCRIPTION: (1) ###\n",
      "--\tDirect and deliver international telecom transformation projects exceeding $80M+ in value, guiding full-lifecycle management from proposal to full service launch in multiple markets.​\n",
      "--\tNegotiate strategic contracts and manage RFP/RFI pipelines, securing high-value enterprise partnerships across the Americas, Europe, and Middle East.​\n",
      "--\tBuild and lead geographically distributed teams (up to 100 members) across operations, integration, and market launch, ensuring excellence in quality and timelines.​\n",
      "--\tArchitect and oversee competitive trials, new product introductions, and complex system integrations resulting in business expansion and client loyalty.​\n",
      "--\tPresent operational status and strategic recommendations to C-level and board audiences, directly communicating delivery risk, revenue implications, and market outcomes.​\n",
      "\n",
      "============================================================\n",
      "\n",
      "### MINIMUM QUALIFICATIONS: (1) ###\n",
      "--\tMaster’s in Management, Engineering, or Telecommunications.​\n",
      "--\tAt least 10 years’ senior project leadership in telecom systems integration and global service delivery.​\n",
      "--\tTrack record in contract negotiation, complex project delivery, and market launch under tight deadlines.\n",
      "\n",
      "============================================================\n",
      "\n",
      "================ SPA-CY FEATURES PER RESUME SECTION ================\n",
      "\n",
      "\n",
      "### ## **PROFESSIONAL EXPERIENCE** (1) ###\n",
      "Noun Chunks:  server bistro, service, + customers, a guest satisfaction rating, 95%, daily specials, premium beverages, a 10% increase, average check, house\n",
      "Compound Nouns:  server bistro, a guest satisfaction rating, premium beverages, a 10% increase\n",
      "Verbal Nouns:  \n",
      "Verbs:  achieve, amount, collaborate, contribute, deliver, present, provide, upsell\n",
      "Dates:  May 2020 - Present Provide, daily\n",
      "Proper Nouns:  Bistro, Chicago, IL, May, Server\n",
      "Numbers:  +, 10, 2020, 56, 80, 95\n",
      "============================================================\n",
      "\n",
      "### SERVICE (1) ###\n",
      "Noun Chunks:  server downtown diner, april, guest orders, a timely manner, consistent positive feedback, guest complaints, issues, high satisfaction levels, cleanliness, organization, dining areas, overall restaurant efficiency\n",
      "Compound Nouns:  server downtown diner, guest orders, guest complaints, high satisfaction levels, dining areas, overall restaurant efficiency\n",
      "Verbal Nouns:  \n",
      "Verbs:  achieve, assist, deliver, ensure, maintain, manage, resolve, support\n",
      "Dates:  June 2017 - April 2020\n",
      "Proper Nouns:  April, Chicago, Diner, Downtown, IL, June, Server\n",
      "Numbers:  2017, 2020\n",
      "============================================================\n",
      "\n",
      "### HIGH SCHOOL DIPLOMA (1) ###\n",
      "Noun Chunks:  chicago high school, il\n",
      "Compound Nouns:  chicago high school\n",
      "Verbal Nouns:  \n",
      "Verbs:  \n",
      "Dates:  May 2017\n",
      "Proper Nouns:  Chicago, High, IL, May, School\n",
      "Numbers:  2017\n",
      "============================================================\n",
      "\n",
      "### CONTACT (1) ###\n",
      "Noun Chunks:  il\n",
      "Compound Nouns:  \n",
      "Verbal Nouns:  \n",
      "Verbs:  \n",
      "Dates:  \n",
      "Proper Nouns:  Chicago, IL, LinkedIn, Portfolio\n",
      "Numbers:  234, 555, 5678, 60601\n",
      "============================================================\n",
      "\n",
      "### CUSTOMER SERVICE (1) ###\n",
      "Noun Chunks:  techniques guest relations\n",
      "Compound Nouns:  techniques guest relations\n",
      "Verbal Nouns:  \n",
      "Verbs:  upselle\n",
      "Dates:  \n",
      "Proper Nouns:  Guest, Time\n",
      "Numbers:  \n",
      "============================================================\n",
      "\n",
      "### CERTIFICATIONS (1) ###\n",
      "Noun Chunks:  servsafe food handler certification\n",
      "Compound Nouns:  servsafe food handler certification\n",
      "Verbal Nouns:  \n",
      "Verbs:  \n",
      "Dates:  May 2018\n",
      "Proper Nouns:  Certification, Food, Handler, May, ServSafe\n",
      "Numbers:  2018\n",
      "============================================================\n",
      "\n",
      "================ SPA-CY FEATURES PER JD SECTION ================\n",
      "\n",
      "\n",
      "### JOB DESCRIPTION: (1) ###\n",
      "Noun Chunks:  international telecom transformation projects, value, full-lifecycle management, proposal, full service launch, strategic contracts, rfp/rfi pipelines, high-value enterprise partnerships, the americas, europe, middle east.​, geographically distributed teams, up to 100 members, operations, integration, market launch, excellence, quality, timelines.​ --\tarchitect, competitive trials, new product introductions, complex system integrations, business expansion, client, c-level and board audiences, delivery risk, revenue implications, market\n",
      "Compound Nouns:  international telecom transformation projects, full-lifecycle management, full service launch, rfp/rfi pipelines, high-value enterprise partnerships, middle east.​, market launch, new product introductions, complex system integrations, business expansion, delivery risk, revenue implications\n",
      "Verbal Nouns:  \n",
      "Verbs:  build, communicate, deliver, direct, distribute, ensure, exceed, guide, lead, manage, negotiate, oversee, result, secure\n",
      "Dates:  loyalty.​\n",
      "Proper Nouns:  Americas, East.​, Europe, Middle, RFI, RFP\n",
      "Numbers:  100, 80M+, up to 100\n",
      "============================================================\n",
      "\n",
      "### MINIMUM QUALIFICATIONS: (1) ###\n",
      "Noun Chunks:  --\tmaster, management, engineering, telecommunications.​, --\tat least 10 years’ senior project leadership, telecom systems integration, global service, --\ttrack record, contract negotiation, complex project delivery, market launch, tight deadlines\n",
      "Compound Nouns:  --\tat least 10 years’ senior project leadership, telecom systems integration, --\ttrack record, contract negotiation, complex project delivery, market launch\n",
      "Verbal Nouns:  \n",
      "Verbs:  \n",
      "Dates:  At least 10 years\n",
      "Proper Nouns:  Engineering, Management, Master, Telecommunications.​, Track\n",
      "Numbers:  10\n",
      "============================================================\n",
      "\n",
      "================ ORDERED TAGGED SEMANTIC EXTRACT (RESUME) ================\n",
      "\n",
      "\n",
      "### ## **PROFESSIONAL EXPERIENCE** (1) ###\n",
      "\n",
      "[Server|proper_nouns]\n",
      "[Bistro|proper_nouns] [56|numbers], [Chicago|proper_nouns], [IL|proper_nouns] | [May|proper_nouns] [2020|numbers] - [Present|verbs]\n",
      "[Provide|verbs] [service|noun_chunks] to [80|numbers][+ customers|noun_chunks] [daily|dates], consistently achieving a guest\n",
      "satisfaction rating of [95|numbers]%\n",
      "Actively [upsell|verbs] [[daily|dates] specials|noun_chunks] and [premium beverages|noun_chunks,compounds], contributing to a [10|numbers]%\n",
      "increase in [average check|noun_chunks] amounts\n",
      "[Collaborate|verbs] with kitchen and front-of-[house|noun_chunks] teams to [deliver|verbs] fast and efficient\n",
      "============================================================\n",
      "\n",
      "### SERVICE (1) ###\n",
      "\n",
      "[Server|proper_nouns]\n",
      "[Downtown|proper_nouns] [Diner|proper_nouns], [Chicago|proper_nouns], [IL|proper_nouns] | [[June|proper_nouns] [2017|numbers] - [April|noun_chunks,proper_nouns] [2020|numbers]|dates]\n",
      "Ensured [guest orders|noun_chunks,compounds] were accurate and delivered in [a timely manner|noun_chunks],\n",
      "achieving [consistent positive feedback|noun_chunks]\n",
      "Assisted in managing [guest complaints|noun_chunks,compounds] and resolving [issues|noun_chunks] promptly to\n",
      "[maintain|verbs] [high satisfaction levels|noun_chunks,compounds]\n",
      "Maintained [cleanliness|noun_chunks] and [organization|noun_chunks] of [dining areas|noun_chunks,compounds], supporting overall\n",
      "restaurant efficiency\n",
      "============================================================\n",
      "\n",
      "### HIGH SCHOOL DIPLOMA (1) ###\n",
      "\n",
      "[[Chicago|proper_nouns] [High|proper_nouns] [School|proper_nouns]|noun_chunks,compounds], [Chicago|proper_nouns], [IL|noun_chunks,proper_nouns] | [[May|proper_nouns] [2017|numbers]|dates]\n",
      "============================================================\n",
      "\n",
      "### CONTACT (1) ###\n",
      "\n",
      "([555|numbers]) [234|numbers]-[5678|numbers]\n",
      "rachelmatthews@email.com\n",
      "[LinkedIn|proper_nouns] | [Portfolio|proper_nouns]\n",
      "[Chicago|proper_nouns], [IL|noun_chunks,proper_nouns] [60601|numbers]\n",
      "============================================================\n",
      "\n",
      "### CUSTOMER SERVICE (1) ###\n",
      "\n",
      "Upselling techniques\n",
      "[Guest|proper_nouns] relations\n",
      "[Time|proper_nouns] management\n",
      "Multi-tasking\n",
      "============================================================\n",
      "\n",
      "### CERTIFICATIONS (1) ###\n",
      "\n",
      "[ServSafe|proper_nouns] [Food|proper_nouns] [Handler|proper_nouns]\n",
      "[Certification|proper_nouns], [[May|proper_nouns] [2018|numbers]|dates]\n",
      "============================================================\n",
      "\n",
      "================ ORDERED TAGGED SEMANTIC EXTRACT (JOB DESCRIPTION) ================\n",
      "\n",
      "\n",
      "### JOB DESCRIPTION: (1) ###\n",
      "\n",
      "--\t[Direct|verbs] and [deliver|verbs] [international telecom transformation projects|noun_chunks,compounds] exceeding $80M+ in [value|noun_chunks], guiding [full-lifecycle management|noun_chunks,compounds] from [proposal|noun_chunks] to [full service launch|noun_chunks,compounds] in multiple markets.​\n",
      "--\t[Negotiate|verbs] [strategic contracts|noun_chunks] and [manage|verbs] [[RFP|proper_nouns]/[RFI|proper_nouns] pipelines|noun_chunks,compounds], securing [high-[value|noun_chunks] enterprise partnerships|noun_chunks,compounds] across [the [Americas|proper_nouns]|noun_chunks], [Europe|noun_chunks,proper_nouns], and [Middle|proper_nouns] East.​\n",
      "--\t[Build|verbs] and [lead|verbs] [geographically distributed teams|noun_chunks] ([[up to [100|numbers]|numbers] members|noun_chunks]) across [operations|noun_chunks], [integration|noun_chunks], and [[market|noun_chunks] launch|noun_chunks,compounds], ensuring [excellence|noun_chunks] in [quality|noun_chunks] and timelines.​\n",
      "--\tArchitect and [oversee|verbs] [competitive trials|noun_chunks], [new product introductions|noun_chunks,compounds], and [complex system integrations|noun_chunks,compounds] resulting in [business expansion|noun_chunks,compounds] and [client|noun_chunks] loyalty.​\n",
      "--\tPresent operational status and strategic recommendations to [C-level and board audiences|noun_chunks], directly communicating [delivery risk|noun_chunks,compounds], [revenue implications|noun_chunks,compounds], and [market|noun_chunks] outcomes.​\n",
      "============================================================\n",
      "\n",
      "### MINIMUM QUALIFICATIONS: (1) ###\n",
      "\n",
      "--\t[Master|proper_nouns]’s in [Management|noun_chunks,proper_nouns], [Engineering|noun_chunks,proper_nouns], or Telecommunications.​\n",
      "--\t[At least [10|numbers] years|dates]’ senior project leadership in [telecom systems integration|noun_chunks,compounds] and [global service|noun_chunks] delivery.​\n",
      "--\t[Track|proper_nouns] record in [contract negotiation|noun_chunks,compounds], [complex project delivery|noun_chunks,compounds], and [market launch|noun_chunks,compounds] under [tight deadlines|noun_chunks].\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pymupdf4llm\n",
    "from docx2python import docx2python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# ================================\n",
    "# UNIVERSAL TEXT EXTRACTION\n",
    "# ================================\n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_pdf_blocks(file_path)\n",
    "    elif ext in [\".docx\", \".doc\"]:\n",
    "        return extract_docx_exact_layout(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Use PDF or DOCX.\")\n",
    "\n",
    "def extract_docx_exact_layout(filepath):\n",
    "    doc_result = docx2python(filepath)\n",
    "    main_content = doc_result.body\n",
    "    all_text = []\n",
    "    for section in main_content:\n",
    "        for row in section:\n",
    "            for cell in row:\n",
    "                for para in cell:\n",
    "                    para_str = para.strip()\n",
    "                    if para_str:\n",
    "                        all_text.append(para_str)\n",
    "    return all_text\n",
    "\n",
    "def extract_column_aware_blocks(pdf_path, column_gap=50):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_blocks = []\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"blocks\", sort=True)\n",
    "        blocks = sorted(blocks, key=lambda b: (b[0], b[1]))\n",
    "        left_col, right_col = [], []\n",
    "        if blocks:\n",
    "            page_width = page.rect.width\n",
    "            center_line = page_width / 2\n",
    "            for b in blocks:\n",
    "                x0, y0, x1, y1, text, *_ = b\n",
    "                if x1 < center_line - column_gap:\n",
    "                    left_col.append((y0, text.strip()))\n",
    "                else:\n",
    "                    right_col.append((y0, text.strip()))\n",
    "            left_col_sorted = [t for _, t in sorted(left_col)]\n",
    "            right_col_sorted = [t for _, t in sorted(right_col)]\n",
    "            combined = right_col_sorted + left_col_sorted\n",
    "            all_blocks.extend([t for t in combined if t])\n",
    "    doc.close()\n",
    "    return all_blocks\n",
    "\n",
    "def extract_pdf_blocks(pdf_path):\n",
    "    blocks_code1 = extract_column_aware_blocks(pdf_path)\n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "    blocks_code2 = [\n",
    "        block.strip()\n",
    "        for block in md_text.split(\"\\n\\n\")\n",
    "        if block.strip() and len(block.strip()) > 5\n",
    "    ]\n",
    "    md_chunks = pymupdf4llm.to_markdown(pdf_path, page_chunks=True)\n",
    "    page_chunk_blocks = [\n",
    "        chunk[\"text\"].strip()\n",
    "        for chunk in md_chunks\n",
    "        if isinstance(chunk, dict) and \"text\" in chunk and chunk[\"text\"].strip()\n",
    "    ]\n",
    "    def jaccard_similarity(s1, s2, threshold=0.7):\n",
    "        set1, set2 = set(s1.lower().strip()), set(s2.lower().strip())\n",
    "        intersection = set1 & set2\n",
    "        union = set1 | set2\n",
    "        if not union:\n",
    "            return False\n",
    "        return len(intersection) / len(union) > threshold\n",
    "    missing_from_code2 = []\n",
    "    for block1 in blocks_code1:\n",
    "        if not any(jaccard_similarity(block1, block2) for block2 in blocks_code2):\n",
    "            missing_from_code2.append(block1)\n",
    "    final_blocks = blocks_code2 + missing_from_code2\n",
    "    return final_blocks, page_chunk_blocks\n",
    "\n",
    "# ================================\n",
    "# SEMANTIC HEADING DETECTION + SECTION GROUPING (Occurrence aware!)\n",
    "# ================================\n",
    "resume_headings = [\n",
    "    \"experience\", \"work experience\", \"education\", \"academic history\",\n",
    "    \"projects\", \"technical projects\", \"skills\", \"key skills\",\n",
    "    \"certifications\", \"achievements\", \"publications\", \"personal details\", \"contact\",\n",
    "    \"technical skills\", \"professional experience\", \"core competencies\", \"community service\",\n",
    "    \"portfolio management project\", \"project director\"\n",
    "]\n",
    "\n",
    "jd_headings = [\n",
    "    \"job position\", \"role title\", \"job responsibilities\", \"responsibilities\",\n",
    "    \"duties\", \"minimum qualifications\", \"requirements\", \"qualifications\",\n",
    "    \"preferred qualifications\", \"desired skills\"\n",
    "]\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "resume_embeddings = model.encode(resume_headings, convert_to_tensor=True)\n",
    "jd_embeddings = model.encode(jd_headings, convert_to_tensor=True)\n",
    "\n",
    "def semantic_heading_detection(lines, heading_list, embeddings, threshold=0.6):\n",
    "    detected_headings = []\n",
    "    for line in lines:\n",
    "        line_clean = line.strip().lower()\n",
    "        if not line_clean or len(line_clean) < 3:\n",
    "            continue\n",
    "        emb = model.encode(line_clean, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(emb, embeddings)[0]\n",
    "        max_score = float(cosine_scores.max())\n",
    "        if max_score >= threshold:\n",
    "            detected_headings.append(line.strip())\n",
    "    return detected_headings\n",
    "\n",
    "def semantic_sectioning(lines, for_jd=False, threshold=0.55):\n",
    "    headings = jd_headings if for_jd else resume_headings\n",
    "    embeddings = jd_embeddings if for_jd else resume_embeddings\n",
    "    detected_headings = semantic_heading_detection(lines, headings, embeddings, threshold)\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    buffer = []\n",
    "    section_counter = {}\n",
    "    for line in lines:\n",
    "        if line.strip() in detected_headings:\n",
    "            if current_section and buffer:\n",
    "                key = current_section\n",
    "                count = section_counter.get(key, 0) + 1\n",
    "                section_counter[key] = count\n",
    "                section_key = f\"{key} ({count})\"\n",
    "                sections[section_key] = \"\\n\".join(buffer).strip()\n",
    "            current_section = line.strip()\n",
    "            buffer = []\n",
    "        else:\n",
    "            buffer.append(line.strip())\n",
    "    if current_section and buffer:\n",
    "        key = current_section\n",
    "        count = section_counter.get(key, 0) + 1\n",
    "        section_counter[key] = count\n",
    "        section_key = f\"{key} ({count})\"\n",
    "        sections[section_key] = \"\\n\".join(buffer).strip()\n",
    "    return sections\n",
    "\n",
    "# ================================\n",
    "# SPA-CY FEATURE EXTRACTION (with numbers)\n",
    "# ================================\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def filter_and_clean_noun_chunks(doc):\n",
    "    seen = set()\n",
    "    clean_chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        text = chunk.text.strip().lower()\n",
    "        if not text or len(text) < 2:\n",
    "            continue\n",
    "        if all(token.is_stop for token in chunk):\n",
    "            continue\n",
    "        if text in seen:\n",
    "            continue\n",
    "        seen.add(text)\n",
    "        clean_chunks.append(text)\n",
    "    return clean_chunks\n",
    "\n",
    "def extract_section_spacy_features(sections):\n",
    "    section_features = {}\n",
    "    for heading, text in sections.items():\n",
    "        flat = \" \".join(line.strip() for line in text.splitlines() if line.strip())\n",
    "        doc = nlp(flat)\n",
    "        noun_chunks = filter_and_clean_noun_chunks(doc)\n",
    "        verbs = sorted(set([t.lemma_ for t in doc if t.pos_ == \"VERB\" and not t.is_stop and len(t.lemma_) > 1]))\n",
    "        compounds = []\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if any(t.dep_ == \"compound\" for t in chunk):\n",
    "                compound_text = chunk.text.strip().lower()\n",
    "                if compound_text not in compounds:\n",
    "                    compounds.append(compound_text)\n",
    "        verbal_nouns = sorted(set([t.text for t in doc if t.tag_ == \"VBG\" and t.pos_ == \"NOUN\"]))\n",
    "        dates = sorted(set([ent.text for ent in doc.ents if ent.label_ == \"DATE\"]))\n",
    "        proper_nouns = sorted(set([t.text for t in doc if t.pos_ == \"PROPN\" and len(t.text.strip()) > 1]))\n",
    "        # Capture numbers (tokens with POS NUM and entities with relevant labels)\n",
    "        numbers = set([t.text for t in doc if t.pos_ == \"NUM\"])\n",
    "        numbers = numbers.union({ent.text for ent in doc.ents if ent.label_ in [\"CARDINAL\", \"QUANTITY\", \"ORDINAL\", \"MONEY\"]})\n",
    "        section_features[heading] = {\n",
    "            \"noun_chunks\": noun_chunks,\n",
    "            \"compounds\": compounds,\n",
    "            \"verbal_nouns\": verbal_nouns,\n",
    "            \"verbs\": verbs,\n",
    "            \"dates\": dates,\n",
    "            \"proper_nouns\": proper_nouns,\n",
    "            \"numbers\": sorted(numbers)\n",
    "        }\n",
    "    return section_features\n",
    "\n",
    "def print_section_parts_of_speech(sections):\n",
    "    for heading, text in sections.items():\n",
    "        print(f\"\\n### PARTS OF SPEECH IN SECTION: {heading.upper()} ###\\n\")\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            print(f\"{token.text}\\t{token.pos_}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# ================================\n",
    "# MAIN - ORDERED TAGGED EXTRACT (now tags numbers!)\n",
    "# ================================\n",
    "def tag_section_features_in_order(sections, section_features):\n",
    "    for heading, text in sections.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\\n\")\n",
    "        lines = text.splitlines()\n",
    "        features = section_features[heading]\n",
    "        # Add \"numbers\" to feat_types!\n",
    "        feat_types = ['noun_chunks', 'compounds', 'verbal_nouns', 'verbs', 'dates', 'proper_nouns', 'numbers']\n",
    "        feat_map = {}\n",
    "        for ft in feat_types:\n",
    "            for val in features.get(ft, []):\n",
    "                val_low = val.lower().strip()\n",
    "                if val_low not in feat_map:\n",
    "                    feat_map[val_low] = []\n",
    "                feat_map[val_low].append(ft)\n",
    "        phrases_sorted = sorted(feat_map.keys(), key=lambda x: -len(x))\n",
    "        for line in lines:\n",
    "            line_str = line.strip()\n",
    "            output = line_str\n",
    "            for phrase in phrases_sorted:\n",
    "                if phrase and phrase in line_str.lower():\n",
    "                    tag = ','.join(feat_map[phrase])\n",
    "                    pat = r'(?i)\\b({})\\b'.format(re.escape(phrase))\n",
    "                    output = re.sub(pat, r'[\\1|{}]'.format(tag), output)\n",
    "            print(output)\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# ================================\n",
    "# MAIN APPLICATION\n",
    "# ================================\n",
    "def process_files(resume_path, jd_path):\n",
    "    resume_extracted = extract_text(resume_path)\n",
    "    if isinstance(resume_extracted, tuple):\n",
    "        resume_blocks, _ = resume_extracted\n",
    "    else:\n",
    "        resume_blocks = resume_extracted\n",
    "\n",
    "    jd_extracted = extract_text(jd_path)\n",
    "    if isinstance(jd_extracted, tuple):\n",
    "        jd_blocks, _ = jd_extracted\n",
    "    else:\n",
    "        jd_blocks = jd_extracted\n",
    "\n",
    "    resume_sections = semantic_sectioning(resume_blocks, for_jd=False)\n",
    "    jd_sections = semantic_sectioning(jd_blocks, for_jd=True)\n",
    "\n",
    "    resume_features = extract_section_spacy_features(resume_sections)\n",
    "    jd_features = extract_section_spacy_features(jd_sections)\n",
    "\n",
    "    print(\"\\n================ RESUME SECTIONS ================\\n\")\n",
    "    for heading, content in resume_sections.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\\n{content}\\n\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n================ JOB DESCRIPTION SECTIONS ================\\n\")\n",
    "    for heading, content in jd_sections.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\\n{content}\\n\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n================ SPA-CY FEATURES PER RESUME SECTION ================\\n\")\n",
    "    for heading, feats in resume_features.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\")\n",
    "        print(\"Noun Chunks: \", \", \".join(feats[\"noun_chunks\"]))\n",
    "        print(\"Compound Nouns: \", \", \".join(feats[\"compounds\"]))\n",
    "        print(\"Verbal Nouns: \", \", \".join(feats[\"verbal_nouns\"]))\n",
    "        print(\"Verbs: \", \", \".join(feats[\"verbs\"]))\n",
    "        print(\"Dates: \", \", \".join(feats[\"dates\"]))\n",
    "        print(\"Proper Nouns: \", \", \".join(feats[\"proper_nouns\"]))\n",
    "        print(\"Numbers: \", \", \".join(feats[\"numbers\"]))\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n================ SPA-CY FEATURES PER JD SECTION ================\\n\")\n",
    "    for heading, feats in jd_features.items():\n",
    "        print(f\"\\n### {heading.upper()} ###\")\n",
    "        print(\"Noun Chunks: \", \", \".join(feats[\"noun_chunks\"]))\n",
    "        print(\"Compound Nouns: \", \", \".join(feats[\"compounds\"]))\n",
    "        print(\"Verbal Nouns: \", \", \".join(feats[\"verbal_nouns\"]))\n",
    "        print(\"Verbs: \", \", \".join(feats[\"verbs\"]))\n",
    "        print(\"Dates: \", \", \".join(feats[\"dates\"]))\n",
    "        print(\"Proper Nouns: \", \", \".join(feats[\"proper_nouns\"]))\n",
    "        print(\"Numbers: \", \", \".join(feats[\"numbers\"]))\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n================ ORDERED TAGGED SEMANTIC EXTRACT (RESUME) ================\\n\")\n",
    "    tag_section_features_in_order(resume_sections, resume_features)\n",
    "\n",
    "    print(\"\\n================ ORDERED TAGGED SEMANTIC EXTRACT (JOB DESCRIPTION) ================\\n\")\n",
    "    tag_section_features_in_order(jd_sections, jd_features)\n",
    "\n",
    "    # If desired, print raw POS tagging too:\n",
    "    # print_section_parts_of_speech(resume_sections)\n",
    "    # print_section_parts_of_speech(jd_sections)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    resume_path = \"3590975-restaurant-server-job-description-resume-example.pdf\"         # Replace with your resume file path\n",
    "    jd_path = \"C:\\\\Users\\\\zenit\\\\Downloads\\\\job description\\\\for 6 word.docx\"   # Replace with your job description file path\n",
    "    process_files(resume_path, jd_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa7115-6760-45ac-98ab-3ecd00eadd52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_kernal",
   "language": "python",
   "name": "spacy_kernal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
